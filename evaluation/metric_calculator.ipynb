{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_file = pd.read_csv(\"new test result/mixprompt/cpu/mixprompt_cpu_epoch_5_test_res.csv\")\n",
    "cpu_file.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "graphic_file = pd.read_csv(\"new test result/mixprompt/graphic/mixprompt_graphic_epoch_5_test_res.csv\")\n",
    "graphic_file.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "hardisk_file = pd.read_csv(\"new test result/mixprompt/hardisk/mixprompt_hardisk_epoch_5_test_res.csv\")\n",
    "hardisk_file.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "ram_file = pd.read_csv(\"new test result/mixprompt/ram/mixprompt_ram_epoch_5_test_res.csv\")\n",
    "ram_file.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "screen_file = pd.read_csv(\"new test result/mixprompt/screen/mixprompt_screen_epoch_5_test_res.csv\")\n",
    "screen_file.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_label=cpu_file['label'].values\n",
    "cpu_pred=cpu_file['prediction'].values\n",
    "\n",
    "graphic_label=graphic_file['label'].values\n",
    "graphic_pred=graphic_file['prediction'].values\n",
    "\n",
    "hardisk_label=hardisk_file['label'].values\n",
    "hardisk_pred=hardisk_file['prediction'].values\n",
    "\n",
    "ram_label=ram_file['label'].values\n",
    "ram_pred=ram_file['prediction'].values\n",
    "\n",
    "screen_label=screen_file['label'].values\n",
    "screen_pred=screen_file['prediction'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu recall score---> 0.45825820085502195\n",
      "graphic recall score---> 0.32448404985892504\n",
      "hardisk recall score---> 0.3235801582911116\n",
      "ram recall score---> 0.255984780984781\n",
      "screen recall score---> 0.21917731280207903\n",
      "\n",
      "cpu precision score---> 0.46767114628850964\n",
      "graphic precision score---> 0.3420619789914396\n",
      "hardisk precision score---> 0.3019455997034182\n",
      "ram precision score---> 0.30052850520082314\n",
      "screen precision score---> 0.3500216880122366\n"
     ]
    }
   ],
   "source": [
    "print('cpu recall score--->',recall_score(cpu_label, cpu_pred, average='macro'))\n",
    "print('graphic recall score--->',recall_score(graphic_label, graphic_pred, average='macro'))\n",
    "print('hardisk recall score--->',recall_score(hardisk_label, hardisk_pred, average='macro'))\n",
    "print('ram recall score--->',recall_score(ram_label, ram_pred, average='macro'))\n",
    "print('screen recall score--->',recall_score(screen_label, screen_pred, average='macro'))\n",
    "print()\n",
    "print('cpu precision score--->',precision_score(cpu_label, cpu_pred, average='macro'))\n",
    "print('graphic precision score--->',precision_score(graphic_label, graphic_pred, average='macro'))\n",
    "print('hardisk precision score--->',precision_score(hardisk_label, hardisk_pred, average='macro'))\n",
    "print('ram precision score--->',precision_score(ram_label, ram_pred, average='macro'))\n",
    "print('screen precision score--->',precision_score(screen_label, screen_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu f1 score---> 0.1838768115942029\n",
      "graphic f1 score---> 0.1892534657869375\n",
      "hardisk f1 score---> 0.14622217427266518\n",
      "ram f1 score---> 0.1345224823485693\n",
      "screen f1 score---> 0.1423982869379015\n"
     ]
    }
   ],
   "source": [
    "print('cpu f1 score--->',f1_score(cpu_label, cpu_pred, average='macro'))\n",
    "\n",
    "print('graphic f1 score--->',f1_score(graphic_label, graphic_pred, average='macro'))\n",
    "\n",
    "print('hardisk f1 score--->',f1_score(hardisk_label, hardisk_pred, average='macro'))\n",
    "\n",
    "print('ram f1 score--->',f1_score(ram_label, ram_pred, average='macro'))\n",
    "\n",
    "print('screen f1 score--->',f1_score(screen_label, screen_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
