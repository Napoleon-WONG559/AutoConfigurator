{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T02:41:43.300150Z","iopub.status.busy":"2023-05-09T02:41:43.299727Z","iopub.status.idle":"2023-05-09T03:48:47.521415Z","shell.execute_reply":"2023-05-09T03:48:47.520373Z","shell.execute_reply.started":"2023-05-09T02:41:43.300046Z"}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import AdamW\n","from transformers import BertForSequenceClassification, BertConfig\n","from sklearn.utils.class_weight import compute_class_weight\n","import csv\n","\n","# specify GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","#device = 'cpu'\n","print(device)\n","\n","df = pd.read_csv(\"/kaggle/input/reviewdata/review_cpu_label_map.csv\")\n","dfn = pd.read_csv(\"/kaggle/input/needdata/need_cpu_label_map.csv\")\n","\n","train_text, val_text, train_labels, val_labels = train_test_split(df['review'], df['cpu_label'], \n","                                                                    random_state=2018, \n","                                                                    test_size=0.1, \n","                                                                    stratify=df['cpu_label'])\n","\n","finetune_text, test_text, finetune_labels, test_labels = train_test_split(dfn['need'], dfn['cpu_label'], \n","                                                                    random_state=2018, \n","                                                                          test_size=0.5)\n","                                                                    #test_size=0.5, \n","                                                                    #stratify=dfn['screen_label'])\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","\n","max_seq_len = 512\n","\n","# tokenize and encode sequences in the training set\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","batch_size = 16\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n","\n","#--------------------------------------------------------------\n","#---------------------fine tune and test ----------------------\n","#---------------------------start------------------------------\n","\n","# tokenize and encode sequences in the training set\n","tokens_finetune = tokenizer.batch_encode_plus(\n","    finetune_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# for train set\n","finetune_seq = torch.tensor(tokens_finetune['input_ids'])\n","finetune_mask = torch.tensor(tokens_finetune['attention_mask'])\n","finetune_y = torch.tensor(finetune_labels.tolist())\n","\n","# for validation set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())\n","\n","batch_size = 16\n","\n","# wrap tensors\n","finetune_data = TensorDataset(finetune_seq, finetune_mask, finetune_y)\n","\n","# sampler for sampling the data during training\n","finetune_sampler = RandomSampler(finetune_data)\n","\n","# dataLoader for train set\n","finetune_dataloader = DataLoader(finetune_data, sampler=finetune_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","test_data = TensorDataset(test_seq, test_mask, test_y)\n","\n","# sampler for sampling the data during training\n","test_sampler = SequentialSampler(test_data)\n","\n","# dataLoader for validation set\n","test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)\n","\n","#----------------------------end-------------------------------\n","#---------------------fine tune and test ----------------------\n","#--------------------------------------------------------------\n","\n","\n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 4, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n","    )\n","\n","# push the model to GPU\n","model = model.to(device)\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = 1e-5)\n","\n","# train\n","epoch=10\n","model.train()\n","for i in range(epoch):\n","    count=0\n","    loss_rec=0\n","    for batch in train_dataloader:\n","        batch = [r.to(device) for r in batch]\n","        inputs, input_mask, labels=batch\n","        output = model(inputs, \\\n","                        token_type_ids=None, \\\n","                        attention_mask=input_mask, \\\n","                        labels=labels)\n","        loss = output['loss']\n","        logits = output['logits']\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        count+=1\n","        loss_rec+=loss\n","        \n","    print('NO.',i,' epoch avg train loss: ',loss_rec/count)\n","    \n","\n","with torch.no_grad():\n","    model.eval()\n","    preds=[]\n","    labels=[]\n","    for batch in val_dataloader:\n","        batch = [r.to(device) for r in batch]\n","        inputs, input_mask, label=batch\n","        output = model(inputs, \\\n","                        token_type_ids=None, \\\n","                        attention_mask=input_mask, \\\n","                        labels=label)\n","        logits = output['logits']\n","        labels.extend(label.cpu().tolist())\n","        preds.extend(torch.argmax(logits,dim=-1).cpu().tolist())\n","    acc=sum([int(i==j) for i,j in zip(preds, labels)])/len(preds)\n","\n","print(\"validation accuracy is : \",acc)\n","\n","# fine tune\n","epoch=20\n","#model.train()\n","for i in range(epoch):\n","    count=0\n","    loss_rec=0\n","    model.train()\n","    for batch in finetune_dataloader:\n","        batch = [r.to(device) for r in batch]\n","        inputs, input_mask, labels=batch\n","        output = model(inputs, \\\n","                        token_type_ids=None, \\\n","                        attention_mask=input_mask, \\\n","                        labels=labels)\n","        loss = output['loss']\n","        logits = output['logits']\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        count+=1\n","        loss_rec+=loss\n","    print('NO.',i,' epoch avg fine tune loss: ',loss_rec/count)\n","\n","    if(i==0 or i==4 or i==9 or i==14 or i==19):\n","        with torch.no_grad():\n","            model.eval()\n","            all_pred=[]#add\n","            preds=[]\n","            labels=[]\n","            for batch in test_dataloader:\n","                batch = [r.to(device) for r in batch]\n","                inputs, input_mask, label=batch\n","                output = model(inputs, \\\n","                        token_type_ids=None, \\\n","                        attention_mask=input_mask, \\\n","                        labels=label)\n","                logits = output['logits']\n","                all_pred.extend(logits.cpu().tolist())#add\n","                labels.extend(label.cpu().tolist())\n","                preds.extend(torch.argmax(logits,dim=-1).cpu().tolist())\n","            acc=sum([int(i==j) for i,j in zip(preds, labels)])/len(preds)\n","        \n","        save_path=\"/kaggle/working/baselinebert_cpu_epoch_\"+str(i+1)+\"_test_res.csv\"\n","        n=len(labels)\n","        record=[]\n","        for j in range(0,n):\n","            tmp={\"index\":j, \"label\":labels[j], \"prediction\":preds[j], \"all_pred\": all_pred[j]}\n","            record.append(tmp)\n","\n","        with open(save_path, 'w', newline='') as csvfile:\n","            fieldnames = ['index', 'label','prediction','all_pred']\n","            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","\n","            writer.writeheader()\n","            writer.writerows(record)\n","        print(save_path)\n","        print(i,\" epoch test accuracy is : \",acc)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
