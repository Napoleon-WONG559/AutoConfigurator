{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-25T03:36:10.005989Z","iopub.status.busy":"2023-05-25T03:36:10.005507Z","iopub.status.idle":"2023-05-25T03:43:01.462148Z","shell.execute_reply":"2023-05-25T03:43:01.459822Z","shell.execute_reply.started":"2023-05-25T03:36:10.005947Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"name":"stdout","output_type":"stream","text":["cuda:0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d45a5ff4f05848518f090e69a413c079","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"061188daf01c49d5b8440f6fa3c9582c","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a7dc73e4b0345bdabc1259085cb6195","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8611c3fe7d540c984e4be8d5ad185a4","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["NO. 0  epoch avg train loss:  tensor(-123.6794, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 1  epoch avg train loss:  tensor(-469.1632, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 2  epoch avg train loss:  tensor(-985.3215, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 3  epoch avg train loss:  tensor(-1684.6146, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 4  epoch avg train loss:  tensor(-2565.7676, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 5  epoch avg train loss:  tensor(-3620.8445, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 6  epoch avg train loss:  tensor(-4838.3188, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 7  epoch avg train loss:  tensor(-6197.4526, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 8  epoch avg train loss:  tensor(-7742.9497, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 9  epoch avg train loss:  tensor(-9445.3848, device='cuda:0', grad_fn=<DivBackward0>)\n","validation accuracy is :  0.49239280774550487\n","NO. 0  epoch avg fine tune loss:  tensor(-10264.9570, device='cuda:0', grad_fn=<DivBackward0>)\n","/kaggle/working/baselinebert_cpu_epoch_1_test_res.csv\n","0  epoch test accuracy is :  0.3383177570093458\n","NO. 1  epoch avg fine tune loss:  tensor(-10413.6982, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 2  epoch avg fine tune loss:  tensor(-10631.6787, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 3  epoch avg fine tune loss:  tensor(-10824.6377, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 4  epoch avg fine tune loss:  tensor(-11015.8994, device='cuda:0', grad_fn=<DivBackward0>)\n","/kaggle/working/baselinebert_cpu_epoch_5_test_res.csv\n","4  epoch test accuracy is :  0.3383177570093458\n","NO. 5  epoch avg fine tune loss:  tensor(-11180.9775, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 6  epoch avg fine tune loss:  tensor(-11397.8994, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 7  epoch avg fine tune loss:  tensor(-11551.9502, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 8  epoch avg fine tune loss:  tensor(-11737.5703, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 9  epoch avg fine tune loss:  tensor(-11825.5059, device='cuda:0', grad_fn=<DivBackward0>)\n","/kaggle/working/baselinebert_cpu_epoch_10_test_res.csv\n","9  epoch test accuracy is :  0.3383177570093458\n","NO. 10  epoch avg fine tune loss:  tensor(-12103.9004, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 11  epoch avg fine tune loss:  tensor(-12265.0889, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 12  epoch avg fine tune loss:  tensor(-12425.2061, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 13  epoch avg fine tune loss:  tensor(-12599.8838, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 14  epoch avg fine tune loss:  tensor(-12765.4805, device='cuda:0', grad_fn=<DivBackward0>)\n","/kaggle/working/baselinebert_cpu_epoch_15_test_res.csv\n","14  epoch test accuracy is :  0.3383177570093458\n","NO. 15  epoch avg fine tune loss:  tensor(-12952.9033, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 16  epoch avg fine tune loss:  tensor(-13217.8789, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 17  epoch avg fine tune loss:  tensor(-13310.1123, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 18  epoch avg fine tune loss:  tensor(-13481.9814, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 19  epoch avg fine tune loss:  tensor(-13630.3330, device='cuda:0', grad_fn=<DivBackward0>)\n","/kaggle/working/baselinebert_cpu_epoch_20_test_res.csv\n","19  epoch test accuracy is :  0.3383177570093458\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import AdamW\n","from transformers import BertForSequenceClassification, BertConfig\n","from sklearn.utils.class_weight import compute_class_weight\n","import csv\n","import torch.nn.functional as F\n","\n","# specify GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","#device = 'cpu'\n","print(device)\n","\n","df = pd.read_csv(\"/kaggle/input/reviewdata/review_cpu_label_map.csv\")\n","dfn = pd.read_csv(\"/kaggle/input/needdata/need_cpu_label_map.csv\")\n","\n","train_text, val_text, train_labels, val_labels = train_test_split(df['review'], df['cpu_label'], \n","                                                                    random_state=2018, \n","                                                                    test_size=0.1, \n","                                                                    stratify=df['cpu_label'])\n","\n","finetune_text, test_text, finetune_labels, test_labels = train_test_split(dfn['need'], dfn['cpu_label'], \n","                                                                    random_state=2018, \n","                                                                          test_size=0.5)\n","                                                                    #test_size=0.5, \n","                                                                    #stratify=dfn['screen_label'])\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","\n","max_seq_len = 512\n","\n","# tokenize and encode sequences in the training set\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","batch_size = 16\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n","\n","#--------------------------------------------------------------\n","#---------------------fine tune and test ----------------------\n","#---------------------------start------------------------------\n","\n","# tokenize and encode sequences in the training set\n","tokens_finetune = tokenizer.batch_encode_plus(\n","    finetune_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# for train set\n","finetune_seq = torch.tensor(tokens_finetune['input_ids'])\n","finetune_mask = torch.tensor(tokens_finetune['attention_mask'])\n","finetune_y = torch.tensor(finetune_labels.tolist())\n","\n","# for validation set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())\n","\n","batch_size = 16\n","\n","# wrap tensors\n","finetune_data = TensorDataset(finetune_seq, finetune_mask, finetune_y)\n","\n","# sampler for sampling the data during training\n","finetune_sampler = RandomSampler(finetune_data)\n","\n","# dataLoader for train set\n","finetune_dataloader = DataLoader(finetune_data, sampler=finetune_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","test_data = TensorDataset(test_seq, test_mask, test_y)\n","\n","# sampler for sampling the data during training\n","test_sampler = SequentialSampler(test_data)\n","\n","# dataLoader for validation set\n","test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)\n","\n","#----------------------------end-------------------------------\n","#---------------------fine tune and test ----------------------\n","#--------------------------------------------------------------\n","\n","class CNN_Text(nn.Module):\n","    \n","    def __init__(self):\n","        super(CNN_Text, self).__init__()\n","        \n","        V = 28996 #args.embed_num\n","        D = 768#args.embed_dim\n","        C = 4#args.class_num\n","        Ci = 1\n","        Co = 32#args.kernel_num\n","        Ks = [4, 16, 64, 128]#args.kernel_sizes\n","\n","        self.embed = nn.Embedding(30522, D)\n","        self.convs = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc1 = nn.Linear(len(Ks) * Co, C)\n","\n","        #if self.args.static:\n","        #    self.embed.weight.requires_grad = False\n","\n","    def forward(self, x):\n","        #print(x)\n","        x = self.embed(x)  # (N, W, D)\n","    \n","        x = x.unsqueeze(1)  # (N, Ci, W, D)\n","\n","        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(N, Co, W), ...]*len(Ks)\n","\n","        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n","\n","        x = torch.cat(x, 1)\n","\n","        x = self.dropout(x)  # (N, len(Ks)*Co)\n","        logit = self.fc1(x)  # (N, C)\n","        return logit\n","    \n","class CNN_NLP(nn.Module):\n","    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\"\"\"\n","    def __init__(self,\n","                 pretrained_embedding=None,\n","                 freeze_embedding=False,\n","                 vocab_size=30522,\n","                 embed_dim=300,\n","                 filter_sizes=[3, 4, 5],\n","                 num_filters=[100, 100, 100],\n","                 num_classes=2,\n","                 dropout=0.5):\n","        \"\"\"\n","        The constructor for CNN_NLP class.\n","\n","        Args:\n","            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n","                shape (vocab_size, embed_dim)\n","            freeze_embedding (bool): Set to False to fine-tune pretraiend\n","                vectors. Default: False\n","            vocab_size (int): Need to be specified when not pretrained word\n","                embeddings are not used.\n","            embed_dim (int): Dimension of word vectors. Need to be specified\n","                when pretrained word embeddings are not used. Default: 300\n","            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n","            num_filters (List[int]): List of number of filters, has the same\n","                length as `filter_sizes`. Default: [100, 100, 100]\n","            n_classes (int): Number of classes. Default: 2\n","            dropout (float): Dropout rate. Default: 0.5\n","        \"\"\"\n","\n","        super(CNN_NLP, self).__init__()\n","        # Embedding layer\n","        if pretrained_embedding is not None:\n","            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n","            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n","                                                          freeze=freeze_embedding)\n","        else:\n","            self.embed_dim = embed_dim\n","            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n","                                          embedding_dim=self.embed_dim,\n","                                          padding_idx=0,\n","                                          max_norm=5.0)\n","        # Conv Network\n","        self.conv1d_list = nn.ModuleList([\n","            nn.Conv1d(in_channels=self.embed_dim,\n","                      out_channels=num_filters[i],\n","                      kernel_size=filter_sizes[i])\n","            for i in range(len(filter_sizes))\n","        ])\n","        # Fully-connected layer and Dropout\n","        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, input_ids):\n","        \"\"\"Perform a forward pass through the network.\n","\n","        Args:\n","            input_ids (torch.Tensor): A tensor of token ids with shape\n","                (batch_size, max_sent_length)\n","\n","        Returns:\n","            logits (torch.Tensor): Output logits with shape (batch_size,\n","                n_classes)\n","        \"\"\"\n","\n","        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n","        x_embed = self.embedding(input_ids).float()\n","\n","        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n","        # Output shape: (b, embed_dim, max_len)\n","        x_reshaped = x_embed.permute(0, 2, 1)\n","\n","        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n","        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n","\n","        # Max pooling. Output shape: (b, num_filters[i], 1)\n","        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n","            for x_conv in x_conv_list]\n","        \n","        # Concatenate x_pool_list to feed the fully connected layer.\n","        # Output shape: (b, sum(num_filters))\n","        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n","                         dim=1)\n","        \n","        # Compute logits. Output shape: (b, n_classes)\n","        logits = self.fc(self.dropout(x_fc))\n","\n","        return logits\n","\n","model = CNN_Text()\n","#model=CNN_NLP(num_classes=4)\n","\n","# push the model to GPU\n","model = model.to(device)\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = 1e-5)\n","\n","\n","# loss function\n","cross_entropy = nn.NLLLoss()\n","\n","# train\n","epoch=10\n","model.train()\n","for i in range(epoch):\n","    count=0\n","    loss_rec=0\n","    for batch in train_dataloader:\n","        batch = [r.to(device) for r in batch]\n","        inputs, input_mask, labels=batch\n","        \n","        #print(next(model.parameters()).device)\n","        #print(inputs.get_device())\n","        \n","        output = model(inputs)\n","        \n","        loss = cross_entropy(output, labels)\n","        \n","        #logits = output['logits']\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        count+=1\n","        loss_rec+=loss\n","        \n","    print('NO.',i,' epoch avg train loss: ',loss_rec/count)\n","    \n","\n","with torch.no_grad():\n","    model.eval()\n","    preds=[]\n","    labels=[]\n","    for batch in val_dataloader:\n","        batch = [r.to(device) for r in batch]\n","        inputs, input_mask, label=batch\n","        \n","        output = model(inputs)\n","        \n","        logits = output\n","        labels.extend(label.cpu().tolist())\n","        preds.extend(torch.argmax(logits,dim=-1).cpu().tolist())\n","    acc=sum([int(i==j) for i,j in zip(preds, labels)])/len(preds)\n","\n","print(\"validation accuracy is : \",acc)\n","\n","# fine tune\n","epoch=20\n","#model.train()\n","for i in range(epoch):\n","    count=0\n","    loss_rec=0\n","    model.train()\n","    for batch in finetune_dataloader:\n","        batch = [r.to(device) for r in batch]\n","        inputs, input_mask, labels=batch\n","        \n","        output = model(inputs)\n","        \n","        loss = cross_entropy(output, labels)\n","        \n","        #logits = output['logits']\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        count+=1\n","        loss_rec+=loss\n","    print('NO.',i,' epoch avg fine tune loss: ',loss_rec/count)\n","\n","    if(i==0 or i==4 or i==9 or i==14 or i==19):\n","        with torch.no_grad():\n","            model.eval()\n","            all_pred=[]#add\n","            preds=[]\n","            labels=[]\n","            for batch in test_dataloader:\n","                batch = [r.to(device) for r in batch]\n","                inputs, input_mask, label=batch\n","                \n","                output = model(inputs)\n","                \n","                #loss = output['loss']\n","                logits = output\n","                all_pred.extend(logits.cpu().tolist())#add\n","                labels.extend(label.cpu().tolist())\n","                preds.extend(torch.argmax(logits,dim=-1).cpu().tolist())\n","            acc=sum([int(i==j) for i,j in zip(preds, labels)])/len(preds)\n","        \n","        save_path=\"/kaggle/working/baselinebert_cpu_epoch_\"+str(i+1)+\"_test_res.csv\"\n","        n=len(labels)\n","        record=[]\n","        for j in range(0,n):\n","            tmp={\"index\":j, \"label\":labels[j], \"prediction\":preds[j], \"all_pred\": all_pred[j]}\n","            record.append(tmp)\n","\n","        with open(save_path, 'w', newline='') as csvfile:\n","            fieldnames = ['index', 'label','prediction','all_pred']\n","            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","\n","            writer.writeheader()\n","            writer.writerows(record)\n","        print(save_path)\n","        print(i,\" epoch test accuracy is : \",acc)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
