{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T03:12:25.193549Z","iopub.status.busy":"2023-05-02T03:12:25.193084Z","iopub.status.idle":"2023-05-02T03:12:29.489053Z","shell.execute_reply":"2023-05-02T03:12:29.487798Z","shell.execute_reply.started":"2023-05-02T03:12:25.193508Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["10\n"]}],"source":["import torch\n","\n","PATH=\"/kaggle/input/pretrainembedding/PTMTLtemplateEmb1.pt\"\n","checkpoint = torch.load(PATH,map_location=torch.device('cpu'))\n","print(checkpoint['epoch'])"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T03:12:31.370228Z","iopub.status.busy":"2023-05-02T03:12:31.369149Z","iopub.status.idle":"2023-05-02T03:12:52.519365Z","shell.execute_reply":"2023-05-02T03:12:52.518040Z","shell.execute_reply.started":"2023-05-02T03:12:31.370186Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting openprompt\n","  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from openprompt) (2.1.0)\n","Requirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from openprompt) (5.0.0)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from openprompt) (1.7.3)\n","Requirement already satisfied: tensorboardX in /opt/conda/lib/python3.7/site-packages (from openprompt) (2.5.1)\n","Requirement already satisfied: transformers>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from openprompt) (4.26.1)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from openprompt) (0.3.6)\n","Requirement already satisfied: tqdm>=4.62.2 in /opt/conda/lib/python3.7/site-packages (from openprompt) (4.64.1)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from openprompt) (3.2.4)\n","Requirement already satisfied: yacs in /opt/conda/lib/python3.7/site-packages (from openprompt) (0.1.8)\n","Collecting rouge==1.0.0\n","  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n","Collecting sentencepiece==0.1.96\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (2021.11.10)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (0.13.2)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (2.28.2)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (4.11.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (0.12.1)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (2023.1.0)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (0.70.14)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (3.8.3)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (0.18.0)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (1.3.5)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (3.2.0)\n","Collecting protobuf<=3.20.1,>=3.8.0\n","  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (0.13.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (4.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (4.4.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (6.0.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (1.3.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (2.1.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (1.3.3)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (22.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (1.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.14)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.10.0->openprompt) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.10.0->openprompt) (3.4)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.10.0->openprompt) (3.11.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->openprompt) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->openprompt) (2022.7.1)\n","Installing collected packages: sentencepiece, rouge, protobuf, openprompt\n","  Attempting uninstall: sentencepiece\n","    Found existing installation: sentencepiece 0.1.97\n","    Uninstalling sentencepiece-0.1.97:\n","      Successfully uninstalled sentencepiece-0.1.97\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 21.12.2 requires cupy-cuda115, which is not installed.\n","tfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.79.0 which is incompatible.\n","tfx-bsl 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n","tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n","onnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","grpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","googleapis-common-protos 1.58.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-videointelligence 2.8.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-spanner 3.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-resource-manager 1.8.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-pubsub 2.14.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-monitoring 2.14.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-dlp 3.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-api-core 1.34.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","apache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed openprompt-1.0.1 protobuf-3.20.1 rouge-1.0.0 sentencepiece-0.1.96\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install openprompt"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T03:25:22.425914Z","iopub.status.busy":"2023-05-02T03:25:22.425510Z","iopub.status.idle":"2023-05-02T03:25:22.445070Z","shell.execute_reply":"2023-05-02T03:25:22.443970Z","shell.execute_reply.started":"2023-05-02T03:25:22.425874Z"},"trusted":true},"outputs":[],"source":["#graphic card classes and label words\n","graphic_classes = [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4\n","]\n","old_graphic_label_words = {\n","    0: ['NVIDIA GeForce GTX 1050', 'NVIDIA GeForce GTX 1050 Ti', 'GTX 1050 Ti', 'NVIDIA GeForce GTX 1060', 'NVIDIA GeForce GTX 1070', '4GB GDDR5 NVIDIA GeForce GTX 1050', 'GTX 1050'],\n","    1: ['AMD Radeon R4', 'radeon r5', 'AMD Radeon R5 Graphics', 'AMD Radeon R7'],\n","    2: ['Intel UHD Graphics 620', 'Intel Iris Plus Graphics 640', 'NVIDIA GeForce 940MX'],\n","    3: ['Intel HD Graphics 3000', 'Intel', 'Intel HD 620 graphics', 'Intel HD Graphics 500', 'Intel HD Graphics 520', 'Intel HD Graphics 620', 'Intel HD Graphics 400', 'Intel Celeron', 'Intel HD Graphics 505', 'AMD Radeon R2', 'Intel HD Graphics 5500', 'Intel HD Graphics', 'Intel?? HD Graphics 620 (up to 2.07 GB)', 'intel 620'],\n","    4: ['Integrated', 'integrated intel hd graphics', 'integrated AMD Radeon R5 Graphics', 'Integrated Graphics', 'Integrated intel hd graphics'],\n","    5: ['others'],\n","}\n","graphic_label_words={\n","    0:['NVIDIA GeForce GTX 1050', 'NVIDIA GeForce GTX 1050 Ti', 'NVIDIA GeForce GTX 1060', 'NVIDIA GeForce GTX 1070', 'NVIDIA GeForce 940MX'],\n","    1:['AMD Radeon R2', 'AMD Radeon R4', 'AMD Radeon R5 Graphics', 'AMD Radeon R7'],\n","    2:['Intel UHD Graphics 620', 'Intel Iris Plus Graphics 640', 'Intel HD Graphics 3000', 'Intel', 'Intel HD 620 graphics', 'Intel HD Graphics 500', 'Intel HD Graphics 520', 'Intel HD Graphics 620', 'Intel HD Graphics 400', 'Intel Celeron', 'Intel HD Graphics 505', 'Intel HD Graphics 5500', 'Intel HD Graphics', 'Intel?? HD Graphics 620 (up to 2.07 GB)', 'intel 620'],\n","    3:['Integrated', 'integrated intel hd graphics', 'integrated AMD Radeon R5 Graphics', 'Integrated Graphics', 'Integrated intel hd graphics'],\n","    4:['others'],\n","}\n","\n","cpu_classes = [\n","    0,\n","    1,\n","    2,\n","    3\n","]\n","cpu_label_words = {\n","    0: ['4 GHz Intel Core i7'],\n","    1: ['3.8 GHz Intel Core i7', '3.8 GHz Core i7 Family', '3.5 GHz Intel Core i7', '3 GHz 8032', '3.5 GHz 8032', '3 GHz AMD A Series', '3.1 GHz Intel Core i5', '3.4 GHz Intel Core i5', '3.6 GHz AMD A Series', '3.5 GHz Intel Core i5', '3 GHz'],\n","    2: ['2.8 GHz Intel Core i7', '2.7 GHz Core i7 7500U', '2.7 GHz Core i7 2.7 GHz', '2.7 GHz Intel Core i7', '2.1 GHz Intel Core i7', '2.2 GHz Intel Core i5', '2.3 GHz Intel Core i5', '2.6 GHz Intel Core i5', '2.5 GHz Intel Core i5', '2.5 GHz Core i5 7200U', '2 GHz None', '2 GHz AMD A Series', '2.7 GHz Intel Core i3', '2.5 GHz Pentium', '2.5 GHz AMD A Series', '2.16 GHz Intel Celeron', '2.16 GHz Athlon 2650e', '2.7 GHz 8032', '2.48 GHz Intel Celeron', '2.4 GHz AMD A Series', '2 GHz Celeron D Processor 360', '2.4 GHz Intel Core i3', '2.3 GHz Intel Core i3', '2.4 GHz Core i3-540', '2.5 GHz Intel Core Duo', '2.2 GHz Intel Core i3', '2.7 GHz AMD A Series', '2.8 GHz 8032', '2.5 GHz Athlon 2650e', '2.9 GHz Intel Celeron', '2 GB'],\n","    3: ['1.5 GHz', '1.8 GHz 8032', '1.8 GHz AMD E Series', '1.7 GHz', '1.1 GHz Intel Celeron', '1.6 GHz Intel Celeron', '1.6 GHz Intel Core 2 Duo', '1.7 GHz Exynos 5000 Series', '1.6 GHz Celeron N3060', '1.6 GHz AMD E Series', '1.1 GHz Pentium', '1.6 GHz', '1.6 GHz Intel Mobile CPU', '1.6 GHz Celeron N3050', '1.8 GHz Intel Core i7', '1.6 GHz Intel Core i5'],\n","}\n","\n","hard_classes = [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5\n","]\n","hard_label_words = {\n","    0: ['2 TB HDD 5400 rpm'],\n","    1: ['1 TB', '1 TB HDD 7200 rpm', '1000 GB Mechanical Hard Drive', '1000 GB Hybrid Drive', '1 TB HDD 5400 rpm', '1024 GB Mechanical Hard Drive', '1 TB serial_ata', '1 TB mechanical_hard_drive', '1128 GB Hybrid'],\n","    2: ['500 GB HDD 5400 rpm', '500 GB mechanical_hard_drive', 'Solid State Drive, 512 GB', '512 GB SSD'],\n","    3: ['256 GB Flash Memory Solid State', '256 GB', '256.00 SSD', '256 GB SSD', '320 GB HDD 5400 rpm'],\n","    4: ['128 GB Flash Memory Solid State', '128 GB SSD'],\n","    5: ['others'],\n","}\n","\n","ram_classes = [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5,\n","    6\n","]\n","ram_label_words = {\n","    0: ['16 GB DDR4', '16 GB LPDDR3_SDRAM', '16 GB SDRAM', '16 GB DDR SDRAM'],\n","    1: ['12 GB', '12 GB DDR3', '12 GB DDR SDRAM'],\n","    2: ['8 GB SDRAM DDR3', '8 GB DDR3 SDRAM', '8 GB DDR4 2666MHz', '8 GB DDR4', '8 GB LPDDR3', '8 GB DDR4 SDRAM', '8 GB DDR4_SDRAM', '8 GB 2-in1 Media Card Reader, USB 3.1, Type-C', '8 GB DDR SDRAM', '8 GB SDRAM DDR4', '8 GB ddr4', '8 GB sdram', '8 GB SDRAM', '8 GB'],\n","    3: ['6 GB SDRAM', '6 GB', '6 GB SDRAM DDR4', '6 GB DDR SDRAM'],\n","    4: ['4 GB LPDDR3_SDRAM', '4 GB SDRAM DDR4', '4 GB ddr3_sdram', '4 GB DDR3', '4 GB SDRAM', '4 GB', '4 GB SDRAM DDR3', '4 GB DDR4', '4 GB DDR3 SDRAM', '4 GB DDR SDRAM'],\n","    5: ['2 GB SDRAM DDR3', '2 GB SDRAM', '2 GB DDR3L SDRAM', '2 GB DDR3 SDRAM'],\n","    6: ['others'],\n","}\n","\n","scre_classes = [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5,\n","    6,\n","    7\n","]\n","scre_label_words = {\n","    0: ['19.5 inches'],\n","    1: ['17.3 inches'],\n","    2: ['15.6 inches'],\n","    3: ['14 inches'],\n","    4: ['13.5 inches', '13.3 inches'],\n","    5: ['12.5 inches', '12.3 inches'],\n","    6: ['11.6 inches'],\n","    7: ['10.1 inches'],\n","}"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T03:25:25.111341Z","iopub.status.busy":"2023-05-02T03:25:25.110890Z","iopub.status.idle":"2023-05-02T04:43:28.065004Z","shell.execute_reply":"2023-05-02T04:43:28.063781Z","shell.execute_reply.started":"2023-05-02T03:25:25.111304Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/transformers/generation_utils.py:27: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n","  FutureWarning,\n"]},{"name":"stdout","output_type":"stream","text":["cuda:0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c7375ead4704498af2a0ab61d02ad94","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4bcc915c8304a0cba53c3903681cbf4","version_major":2,"version_minor":0},"text/plain":["Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/436M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6c48acf19ec4b7da6b5741b69b44942","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c5f2ad997056482dbda4fe7ab9590f36","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["tokenizing: 0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (786 > 512). Running this sequence through the model will result in indexing errors\n","tokenizing: 6504it [00:15, 423.95it/s]\n","tokenizing: 723it [00:01, 444.01it/s]\n","tokenizing: 535it [00:00, 884.45it/s]\n","tokenizing: 535it [00:00, 847.85it/s]\n","/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]},{"name":"stdout","output_type":"stream","text":["read checkpoint successfully...\n","NO. 0  epoch avg train loss:  tensor(1.4282, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 1  epoch avg train loss:  tensor(1.1882, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 2  epoch avg train loss:  tensor(1.0186, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 3  epoch avg train loss:  tensor(0.8298, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 4  epoch avg train loss:  tensor(0.6420, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 5  epoch avg train loss:  tensor(0.4991, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 6  epoch avg train loss:  tensor(0.4011, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 7  epoch avg train loss:  tensor(0.3398, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 8  epoch avg train loss:  tensor(0.2904, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 9  epoch avg train loss:  tensor(0.2603, device='cuda:0', grad_fn=<DivBackward0>)\n","validation accuracy is :  0.5615491009681881\n","NO. 0  epoch avg fine tune loss:  tensor(1.2274, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 1  epoch avg fine tune loss:  tensor(0.6478, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 2  epoch avg fine tune loss:  tensor(0.4906, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 3  epoch avg fine tune loss:  tensor(0.3833, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 4  epoch avg fine tune loss:  tensor(0.2852, device='cuda:0', grad_fn=<DivBackward0>)\n","/kaggle/working/PTBERT_screen_epoch_5_test_res.csv\n","4  epoch test accuracy is :  0.7065420560747664\n","NO. 5  epoch avg fine tune loss:  tensor(0.2119, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 6  epoch avg fine tune loss:  tensor(0.1625, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 7  epoch avg fine tune loss:  tensor(0.1134, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 8  epoch avg fine tune loss:  tensor(0.0907, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 9  epoch avg fine tune loss:  tensor(0.0764, device='cuda:0', grad_fn=<DivBackward0>)\n","/kaggle/working/PTBERT_screen_epoch_10_test_res.csv\n","9  epoch test accuracy is :  0.685981308411215\n","NO. 10  epoch avg fine tune loss:  tensor(0.0823, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 14  epoch avg fine tune loss:  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n","/kaggle/working/PTBERT_screen_epoch_15_test_res.csv\n","14  epoch test accuracy is :  0.719626168224299\n","NO. 15  epoch avg fine tune loss:  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 16  epoch avg fine tune loss:  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 17  epoch avg fine tune loss:  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 18  epoch avg fine tune loss:  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 19  epoch avg fine tune loss:  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n","/kaggle/working/PTBERT_screen_epoch_20_test_res.csv\n","19  epoch test accuracy is :  0.7177570093457943\n"]}],"source":["import torch\n","from openprompt.data_utils import InputExample\n","import csv\n","from openprompt.plms import load_plm\n","from openprompt.prompts import MixedTemplate\n","from transformers.utils.dummy_pt_objects import PreTrainedModel\n","from openprompt.prompts import ManualVerbalizer\n","from openprompt import PromptForClassification\n","from openprompt import PromptDataLoader\n","from transformers.tokenization_utils import PreTrainedTokenizer\n","import torch.nn as nn\n","from transformers import AdamW\n","from torch.utils.data import random_split\n","from openprompt.prompts import ManualTemplate\n","\n","\n","def read_data_csv(file,ratio):\n","    record=[]\n","    with open(file,newline='') as csvfile:\n","        read=csv.reader(csvfile)\n","        for item in read:\n","            record.append(item[1:])\n","    record=record[1:]\n","    for ind,sample in enumerate(record):\n","        sample.insert(0,ind)\n","        sample[2]=int(sample[2])\n","    train_set, valid_set=random_split(record,\n","                ratio,\n","                 generator=torch.Generator().manual_seed(42))\n","    dataset={}\n","    train_dataset=[]\n","    valid_dataset=[]\n","    for item in train_set:\n","        train_dataset.append(InputExample(guid=item[0],text_a=item[1],label=item[2]))\n","    for item in valid_set:\n","        valid_dataset.append(InputExample(guid=item[0],text_a=item[1],label=item[2]))\n","    dataset['train']=train_dataset\n","    dataset['valid']=valid_dataset\n","    return dataset\n","\n","\n","\n","\n","class MixTemplateModel(nn.Module):\n","    def __init__(self,\n","                plm:PreTrainedModel,\n","                tokenizer: PreTrainedTokenizer,\n","                WrapperClass,\n","                dataset,\n","                needdata,\n","                classes,\n","                epoch,\n","                template_text,\n","                label_words,\n","                device,\n","                ):\n","        \n","        super().__init__()\n","        self.promptTemplate = MixedTemplate(\n","            model=plm,\n","            text = template_text,\n","            tokenizer = tokenizer,\n","        )\n","        #self.promptTemplate = ManualTemplate(\n","        #    text = template_text,\n","        #    tokenizer = tokenizer,\n","        #)\n","\n","        self.promptVerbalizer = ManualVerbalizer(\n","            classes = classes,\n","            label_words = label_words,\n","            tokenizer = tokenizer,\n","            multi_token_handler=\"first\",\n","        )\n","\n","        self.promptModel = PromptForClassification(\n","            template = self.promptTemplate,\n","            plm = plm,\n","            verbalizer = self.promptVerbalizer,\n","        )\n","        self.promptModel.to(device)\n","\n","        #train_set, valid_set=random_split(dataset,\n","        #                                  [0.7,0.3],\n","        #                                  generator=torch.Generator().manual_seed(42))\n","        train_set=dataset['train']\n","        valid_set=dataset['valid']\n","        \n","        finetune_set=needdata['train']\n","        test_set=needdata['valid']\n","\n","        self.train_data_loader = PromptDataLoader(\n","            dataset = train_set,\n","            tokenizer = tokenizer,\n","            template = self.promptTemplate,\n","            tokenizer_wrapper_class=WrapperClass,\n","            batch_size=16,\n","            shuffle=True,\n","            #max_seq_length=800,\n","        )\n","        self.valid_data_loader = PromptDataLoader(\n","            dataset = valid_set,\n","            tokenizer = tokenizer,\n","            template = self.promptTemplate,\n","            tokenizer_wrapper_class=WrapperClass,\n","            batch_size=16,\n","            #max_seq_length=800,\n","        )\n","        \n","        self.finetune_data_loader = PromptDataLoader(\n","            dataset = finetune_set,\n","            tokenizer = tokenizer,\n","            template = self.promptTemplate,\n","            tokenizer_wrapper_class=WrapperClass,\n","            batch_size=16,\n","            shuffle=True,\n","            #max_seq_length=800,\n","        )\n","        self.test_data_loader = PromptDataLoader(\n","            dataset = test_set,\n","            tokenizer = tokenizer,\n","            template = self.promptTemplate,\n","            tokenizer_wrapper_class=WrapperClass,\n","            batch_size=16,\n","            #max_seq_length=800,\n","        )\n","\n","        self.cross_entropy  = nn.NLLLoss()\n","        no_decay = ['bias', 'LayerNorm.weight']\n","        # it's always good practice to set no decay to biase and LayerNorm parameters\n","        optimizer_grouped_parameters1 = [\n","            {'params': [p for n, p in self.promptModel.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","            {'params': [p for n, p in self.promptModel.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","        # Using different optimizer for prompt parameters and model parameters\n","        optimizer_grouped_parameters2 = [\n","            {'params': [p for n,p in self.promptModel.template.named_parameters() if \"raw_embedding\" not in n]}\n","        ]\n","        self.optimizer1 = AdamW(optimizer_grouped_parameters1, lr=1e-5)\n","        self.optimizer2 = AdamW(optimizer_grouped_parameters2, lr=1e-5)\n","\n","        self.epoch=epoch\n","\n","    def forward(self,batch):\n","        outputs=self.promptModel(batch)\n","\n","        return outputs\n","\n","    def train(self):\n","        self.promptModel.train()\n","\n","    def eval(self):\n","        self.promptModel.eval()\n","    \n","    def set_epoch(self,epoch):\n","        self.epoch=epoch\n","\n","\n","\n","\n","\n","\n","if __name__ == '__main__':\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    print(device)\n","\n","    #load pre-trained model\n","    graphic_plm, graphic_tokenizer, graphic_model_config, graphic_WrapperClass = load_plm(\"bert\", \"bert-base-cased\")\n","\n","    #graphic model\n","    graphic_dataset=read_data_csv(\"/kaggle/input/reviewdata/review_screen_label_map.csv\",[6504,723])\n","    #graphic_train_set, graphic_valid_set=random_split(graphic_dataset,\n","    #                                                  [0.7,0.3],\n","    #                                                  generator=torch.Generator().manual_seed(42))\n","    graphic_need_dataset=read_data_csv(\"/kaggle/input/needdata/need_screen_label_map.csv\",[535,535])\n","    graphic_epoch=10\n","    graphic_template='{\"soft\": \"Someone said : \"} {\"placeholder\":\"text_a\"} {\"soft\": \"So he need\"} a computer with a {\"mask\"} {\"soft\": \"configuration\"}'\n","    #graphic_template='{\"placeholder\":\"text_a\"} {\"mask\"}'\n","    #graphic_template='{\"soft\": \"Someone want to buy a computer and he said that\"} {\"placeholder\":\"text_a\"} So he want {\"mask\"}'\n","    graphic_model=MixTemplateModel(graphic_plm,\n","                                   graphic_tokenizer,\n","                                   graphic_WrapperClass,\n","                                   graphic_dataset,\n","                                   graphic_need_dataset,\n","                                   scre_classes,\n","                                   graphic_epoch,\n","                                   graphic_template,\n","                                   scre_label_words,\n","                                   device)\n","\n","    #read pre-trained embedding\n","    graphic_model.promptModel.template.load_state_dict(checkpoint['model_state_dict'])\n","    print(\"read checkpoint successfully...\")\n","    \n","    #-----------------------Train-------------------------\n","    graphic_model.train()\n","    for i in range(graphic_model.epoch):\n","        count=0\n","        loss_rec=0\n","        for batch in graphic_model.train_data_loader:\n","            batch.to(device)\n","            graphic_labels=batch['label']\n","            graphic_logits=graphic_model(batch)\n","            graphic_loss=graphic_model.cross_entropy(graphic_logits,graphic_labels)\n","            graphic_loss.backward()\n","            graphic_model.optimizer1.step()\n","            graphic_model.optimizer1.zero_grad()\n","            graphic_model.optimizer2.step()\n","            graphic_model.optimizer2.zero_grad()\n","            count+=1\n","            loss_rec+=graphic_loss\n","        print('NO.',i,' epoch avg train loss: ',loss_rec/count)\n","    \n","    #-----------------------Validate-------------------------\n","    with torch.no_grad():\n","        graphic_model.eval()\n","        preds=[]\n","        labels=[]\n","        for step, inputs in enumerate(graphic_model.valid_data_loader):\n","            inputs.to(device)\n","            graphic_logits=graphic_model(inputs)\n","            graphic_label=inputs['label']\n","            labels.extend(graphic_label.cpu().tolist())\n","            preds.extend(torch.argmax(graphic_logits,dim=-1).cpu().tolist())\n","        acc=sum([int(i==j) for i,j in zip(preds, labels)])/len(preds)\n","\n","    print(\"validation accuracy is : \",acc)\n","\n","    \n","    #for i in range(graphic_model.epoch):\n","    for i in range(20):\n","        count=0\n","        loss_rec=0\n","        graphic_model.train()\n","        #--------------------fine tune----------------------\n","        for batch in graphic_model.finetune_data_loader:\n","            batch.to(device)\n","            graphic_labels=batch['label']\n","            graphic_logits=graphic_model(batch)\n","            graphic_loss=graphic_model.cross_entropy(graphic_logits,graphic_labels)\n","            graphic_loss.backward()\n","            graphic_model.optimizer1.step()\n","            graphic_model.optimizer1.zero_grad()\n","            graphic_model.optimizer2.step()\n","            graphic_model.optimizer2.zero_grad()\n","            count+=1\n","            loss_rec+=graphic_loss\n","        print('NO.',i,' epoch avg fine tune loss: ',loss_rec/count)\n","    \n","        #-----------------------test-------------------------\n","        if(i==4 or i==9 or i==14 or i==19):\n","            with torch.no_grad():\n","                graphic_model.eval()\n","                all_pred=[]#add\n","                preds=[]\n","                labels=[]\n","                for step, inputs in enumerate(graphic_model.test_data_loader):\n","                    inputs.to(device)\n","                    graphic_logits=graphic_model(inputs)\n","                    graphic_label=inputs['label']\n","                    #all_pred.append(graphic_logits.cpu())#add\n","                    all_pred.extend(graphic_logits.cpu().tolist())#add\n","                    labels.extend(graphic_label.cpu().tolist())\n","                    preds.extend(torch.argmax(graphic_logits,dim=-1).cpu().tolist())\n","                acc=sum([int(i==j) for i,j in zip(preds, labels)])/len(preds)\n","\n","            save_path=\"/kaggle/working/PTBERT_screen_epoch_\"+str(i+1)+\"_test_res.csv\"\n","            n=len(labels)\n","            record=[]\n","            for j in range(0,n):\n","                tmp={\"index\":j, \"label\":labels[j], \"prediction\":preds[j], \"all_pred\": all_pred[j]}\n","                record.append(tmp)\n","\n","            with open(save_path, 'w', newline='') as csvfile:\n","                fieldnames = ['index', 'label','prediction','all_pred']\n","                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","\n","                writer.writeheader()\n","                writer.writerows(record)\n","            print(save_path)\n","            print(i,\" epoch test accuracy is : \",acc)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
