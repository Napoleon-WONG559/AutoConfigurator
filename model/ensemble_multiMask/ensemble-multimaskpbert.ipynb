{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install openprompt","metadata":{"execution":{"iopub.status.busy":"2023-07-04T08:26:00.274545Z","iopub.execute_input":"2023-07-04T08:26:00.274986Z","iopub.status.idle":"2023-07-04T08:26:15.878152Z","shell.execute_reply.started":"2023-07-04T08:26:00.274948Z","shell.execute_reply":"2023-07-04T08:26:15.876914Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting openprompt\n  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers>=4.10.0 in /opt/conda/lib/python3.10/site-packages (from openprompt) (4.30.1)\nCollecting sentencepiece==0.1.96 (from openprompt)\n  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.62.2 in /opt/conda/lib/python3.10/site-packages (from openprompt) (4.64.1)\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (from openprompt) (2.6)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from openprompt) (3.2.4)\nCollecting yacs (from openprompt)\n  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from openprompt) (0.3.6)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from openprompt) (2.1.0)\nCollecting rouge==1.0.0 (from openprompt)\n  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from openprompt) (11.0.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from openprompt) (1.10.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge==1.0.0->openprompt) (1.16.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (0.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (0.3.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->openprompt) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->openprompt) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->openprompt) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets->openprompt) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->openprompt) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->openprompt) (0.18.0)\nRequirement already satisfied: protobuf<4,>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from tensorboardX->openprompt) (3.20.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.10.0->openprompt) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.10.0->openprompt) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.10.0->openprompt) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->openprompt) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->openprompt) (2023.3)\nInstalling collected packages: sentencepiece, yacs, rouge, openprompt\n  Attempting uninstall: sentencepiece\n    Found existing installation: sentencepiece 0.1.99\n    Uninstalling sentencepiece-0.1.99:\n      Successfully uninstalled sentencepiece-0.1.99\nSuccessfully installed openprompt-1.0.1 rouge-1.0.0 sentencepiece-0.1.96 yacs-0.1.8\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom openprompt.data_utils import InputExample\nimport csv\nfrom openprompt.plms import load_plm\nfrom openprompt.prompts import MixedTemplate\nfrom transformers.utils.dummy_pt_objects import PreTrainedModel\nfrom openprompt.prompts import ManualVerbalizer\nfrom openprompt import PromptForClassification\nfrom openprompt import PromptDataLoader\nfrom transformers.tokenization_utils import PreTrainedTokenizer\nimport torch.nn as nn\nfrom transformers import AdamW\nfrom torch.utils.data import random_split\nfrom openprompt.prompts import ManualTemplate\nimport gc\n\n#graphic card classes and label words\ngraphic_classes = [\n    0,\n    1,\n    2,\n    3,\n    4\n]\ngraphic_label_words={\n    0:['NVIDIA GeForce GTX 1050', 'NVIDIA GeForce GTX 1050 Ti', 'NVIDIA GeForce GTX 1060', 'NVIDIA GeForce GTX 1070', 'NVIDIA GeForce 940MX'],\n    1:['AMD Radeon R2', 'AMD Radeon R4', 'AMD Radeon R5 Graphics', 'AMD Radeon R7'],\n    2:['Intel UHD Graphics 620', 'Intel Iris Plus Graphics 640', 'Intel HD Graphics 3000', 'Intel', 'Intel HD 620 graphics', 'Intel HD Graphics 500', 'Intel HD Graphics 520', 'Intel HD Graphics 620', 'Intel HD Graphics 400', 'Intel Celeron', 'Intel HD Graphics 505', 'Intel HD Graphics 5500', 'Intel HD Graphics', 'Intel?? HD Graphics 620 (up to 2.07 GB)', 'intel 620'],\n    3:['Integrated', 'integrated intel hd graphics', 'integrated AMD Radeon R5 Graphics', 'Integrated Graphics', 'Integrated intel hd graphics'],\n    4:['others'],\n}\ncpu_classes = [\n    0,\n    1,\n    2,\n    3\n]\ncpu_label_words = {\n    0: ['4 GHz Intel Core i7'],\n    1: ['3.8 GHz Intel Core i7', '3.8 GHz Core i7 Family', '3.5 GHz Intel Core i7', '3 GHz 8032', '3.5 GHz 8032', '3 GHz AMD A Series', '3.1 GHz Intel Core i5', '3.4 GHz Intel Core i5', '3.6 GHz AMD A Series', '3.5 GHz Intel Core i5', '3 GHz'],\n    2: ['2.8 GHz Intel Core i7', '2.7 GHz Core i7 7500U', '2.7 GHz Core i7 2.7 GHz', '2.7 GHz Intel Core i7', '2.1 GHz Intel Core i7', '2.2 GHz Intel Core i5', '2.3 GHz Intel Core i5', '2.6 GHz Intel Core i5', '2.5 GHz Intel Core i5', '2.5 GHz Core i5 7200U', '2 GHz None', '2 GHz AMD A Series', '2.7 GHz Intel Core i3', '2.5 GHz Pentium', '2.5 GHz AMD A Series', '2.16 GHz Intel Celeron', '2.16 GHz Athlon 2650e', '2.7 GHz 8032', '2.48 GHz Intel Celeron', '2.4 GHz AMD A Series', '2 GHz Celeron D Processor 360', '2.4 GHz Intel Core i3', '2.3 GHz Intel Core i3', '2.4 GHz Core i3-540', '2.5 GHz Intel Core Duo', '2.2 GHz Intel Core i3', '2.7 GHz AMD A Series', '2.8 GHz 8032', '2.5 GHz Athlon 2650e', '2.9 GHz Intel Celeron', '2 GB'],\n    3: ['1.5 GHz', '1.8 GHz 8032', '1.8 GHz AMD E Series', '1.7 GHz', '1.1 GHz Intel Celeron', '1.6 GHz Intel Celeron', '1.6 GHz Intel Core 2 Duo', '1.7 GHz Exynos 5000 Series', '1.6 GHz Celeron N3060', '1.6 GHz AMD E Series', '1.1 GHz Pentium', '1.6 GHz', '1.6 GHz Intel Mobile CPU', '1.6 GHz Celeron N3050', '1.8 GHz Intel Core i7', '1.6 GHz Intel Core i5'],\n}\nhard_classes = [\n    0,\n    1,\n    2,\n    3,\n    4,\n    5\n]\nhard_label_words = {\n    0: ['2 TB HDD 5400 rpm'],\n    1: ['1 TB', '1 TB HDD 7200 rpm', '1000 GB Mechanical Hard Drive', '1000 GB Hybrid Drive', '1 TB HDD 5400 rpm', '1024 GB Mechanical Hard Drive', '1 TB serial_ata', '1 TB mechanical_hard_drive', '1128 GB Hybrid'],\n    2: ['500 GB HDD 5400 rpm', '500 GB mechanical_hard_drive', 'Solid State Drive, 512 GB', '512 GB SSD'],\n    3: ['256 GB Flash Memory Solid State', '256 GB', '256.00 SSD', '256 GB SSD', '320 GB HDD 5400 rpm'],\n    4: ['128 GB Flash Memory Solid State', '128 GB SSD'],\n    5: ['others'],\n}\nram_classes = [\n    0,\n    1,\n    2,\n    3,\n    4,\n    5,\n    6\n]\nram_label_words = {\n    0: ['16 GB DDR4', '16 GB LPDDR3_SDRAM', '16 GB SDRAM', '16 GB DDR SDRAM'],\n    1: ['12 GB', '12 GB DDR3', '12 GB DDR SDRAM'],\n    2: ['8 GB SDRAM DDR3', '8 GB DDR3 SDRAM', '8 GB DDR4 2666MHz', '8 GB DDR4', '8 GB LPDDR3', '8 GB DDR4 SDRAM', '8 GB DDR4_SDRAM', '8 GB 2-in1 Media Card Reader, USB 3.1, Type-C', '8 GB DDR SDRAM', '8 GB SDRAM DDR4', '8 GB ddr4', '8 GB sdram', '8 GB SDRAM', '8 GB'],\n    3: ['6 GB SDRAM', '6 GB', '6 GB SDRAM DDR4', '6 GB DDR SDRAM'],\n    4: ['4 GB LPDDR3_SDRAM', '4 GB SDRAM DDR4', '4 GB ddr3_sdram', '4 GB DDR3', '4 GB SDRAM', '4 GB', '4 GB SDRAM DDR3', '4 GB DDR4', '4 GB DDR3 SDRAM', '4 GB DDR SDRAM'],\n    5: ['2 GB SDRAM DDR3', '2 GB SDRAM', '2 GB DDR3L SDRAM', '2 GB DDR3 SDRAM'],\n    6: ['others'],\n}\nscre_classes = [\n    0,\n    1,\n    2,\n    3,\n    4,\n    5,\n    6,\n    7\n]\nscre_label_words = {\n    0: ['19.5 inches'],\n    1: ['17.3 inches'],\n    2: ['15.6 inches'],\n    3: ['14 inches'],\n    4: ['13.5 inches', '13.3 inches'],\n    5: ['12.5 inches', '12.3 inches'],\n    6: ['11.6 inches'],\n    7: ['10.1 inches'],\n}\n\ndef copy_batch(batch,device):\n    n_batch={}\n    for key in batch.keys():\n        ni=torch.tensor(batch[key],device=device)\n        n_batch[key]=ni\n    return n_batch\n        \n\ndef read_data_csv(file,ratio):\n    record=[]\n    with open(file,newline='') as csvfile:\n        read=csv.reader(csvfile)\n        for item in read:\n            record.append(item[1:])\n    record=record[1:]\n    for ind,sample in enumerate(record):\n        sample.insert(0,ind)\n        sample[2]=int(sample[2])#cpu\n        sample[3]=int(sample[3])#graphic\n        sample[4]=int(sample[4])#hardisk\n        sample[5]=int(sample[5])#ram\n        sample[6]=int(sample[6])#screen\n        \n    \"\"\"train_set, valid_set=random_split(record,\n                 #[0.7,0.3],\n                 ratio,\n                 generator=torch.Generator().manual_seed(42))\"\"\"\n    \n    dataset={}\n    train_dataset=[]\n    valid_dataset=[]\n    \"\"\"for item in train_set:\n        train_dataset.append(InputExample(guid=item[0],text_a=item[1],label=item[2:]))\n    for item in valid_set:\n        valid_dataset.append(InputExample(guid=item[0],text_a=item[1],label=item[2:]))\"\"\"\n    for item in record:\n        train_dataset.append(InputExample(guid=item[0],text_a=item[1],label=item[2:]))\n    dataset['train']=train_dataset\n    dataset['valid']=valid_dataset\n    return dataset\n\n\n\n\nclass multiMask_MixTemplateModel(nn.Module):\n    def __init__(self,\n                plm:PreTrainedModel,\n                tokenizer: PreTrainedTokenizer,\n                WrapperClass,\n                dataset,\n                needdata,\n                #classes,\n                epoch,\n                template_text,\n                #shareTemplate,\n                #label_words,\n                device,\n                cpu_classes,\n                cpu_label_words,\n                graphic_classes,\n                grapihc_label_words,\n                hardisk_classes,\n                hardisk_label_words,\n                ram_classes,\n                ram_label_words,\n                screen_classes,\n                screen_label_words,\n                ):\n        \n        super().__init__()\n        \n        #self.promptTemplate = shareTemplate\n        self.promptTemplate = MixedTemplate(\n            model=plm,\n            text = template_text,\n            tokenizer = tokenizer,\n        )\n\n        # 5 verbalizer correpond to 5 attributes(cpu, graphic card, hard disk, ram, screen)\n        self.cpu_promptVerbalizer = ManualVerbalizer(\n            classes = cpu_classes,\n            label_words = cpu_label_words,\n            tokenizer = tokenizer,\n            multi_token_handler=\"first\",\n        )\n        self.cpu_promptVerbalizer.to(device)\n        self.graphic_promptVerbalizer = ManualVerbalizer(\n            classes = graphic_classes,\n            label_words = graphic_label_words,\n            tokenizer = tokenizer,\n            multi_token_handler=\"first\",\n        )\n        self.graphic_promptVerbalizer.to(device)\n        self.hardisk_promptVerbalizer = ManualVerbalizer(\n            classes = hardisk_classes,\n            label_words = hardisk_label_words,\n            tokenizer = tokenizer,\n            multi_token_handler=\"first\",\n        )\n        self.hardisk_promptVerbalizer.to(device)\n        self.ram_promptVerbalizer = ManualVerbalizer(\n            classes = ram_classes,\n            label_words = ram_label_words,\n            tokenizer = tokenizer,\n            multi_token_handler=\"first\",\n        )\n        self.ram_promptVerbalizer.to(device)\n        self.screen_promptVerbalizer = ManualVerbalizer(\n            classes = screen_classes,\n            label_words = screen_label_words,\n            tokenizer = tokenizer,\n            multi_token_handler=\"first\",\n        )\n        self.screen_promptVerbalizer.to(device)\n\n        # Model backbone\n        self.promptModel = PromptForClassification(\n            template = self.promptTemplate,\n            plm = plm,\n            #verbalizer = self.promptVerbalizer,\n            verbalizer = None,#rewrite model foward function and use the verbalizer outside\n        )\n        self.promptModel.to(device)\n\n        #train_set, valid_set=random_split(dataset,\n        #                                  [0.7,0.3],\n        #                                  generator=torch.Generator().manual_seed(42))\n        train_set=dataset['train']\n        valid_set=dataset['valid']\n        \n        finetune_set=needdata['train']\n        test_set=needdata['valid']\n\n        self.train_data_loader = PromptDataLoader(\n            dataset = train_set,\n            tokenizer = tokenizer,\n            template = self.promptTemplate,\n            tokenizer_wrapper_class=WrapperClass,\n            batch_size=16,\n            shuffle=True,\n            #max_seq_length=800,\n        )\n        \"\"\"self.valid_data_loader = PromptDataLoader(\n            dataset = valid_set,\n            tokenizer = tokenizer,\n            template = self.promptTemplate,\n            tokenizer_wrapper_class=WrapperClass,\n            batch_size=16,\n            #max_seq_length=800,\n        )\"\"\"\n        \n        self.finetune_data_loader = PromptDataLoader(\n            dataset = finetune_set,\n            tokenizer = tokenizer,\n            template = self.promptTemplate,\n            tokenizer_wrapper_class=WrapperClass,\n            batch_size=16,\n            shuffle=True,\n            #max_seq_length=800,\n        )\n        self.test_data_loader = PromptDataLoader(\n            dataset = test_set,\n            tokenizer = tokenizer,\n            template = self.promptTemplate,\n            tokenizer_wrapper_class=WrapperClass,\n            batch_size=16,\n            #max_seq_length=800,\n        )\n\n        self.cross_entropy  = nn.NLLLoss()\n        no_decay = ['bias', 'LayerNorm.weight']\n        # it's always good practice to set no decay to biase and LayerNorm parameters\n        optimizer_grouped_parameters1 = [\n            {'params': [p for n, p in self.promptModel.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n            {'params': [p for n, p in self.promptModel.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n        # Using different optimizer for prompt parameters and model parameters\n        optimizer_grouped_parameters2 = [\n            {'params': [p for n,p in self.promptModel.template.named_parameters() if \"raw_embedding\" not in n]}\n        ]\n        self.optimizer1 = AdamW(optimizer_grouped_parameters1, lr=1e-5)\n        self.optimizer2 = AdamW(optimizer_grouped_parameters2, lr=1e-5)\n\n        self.epoch=epoch\n\n    def forward(self,batch):\n        \"\"\"outputs=self.promptModel(batch)\"\"\"\n        #rewrite forward function\n        \n        outputs = self.promptModel.prompt_model(batch)\n        \n        #outputs = self.verbalizer.gather_outputs(outputs)\n        outputs=outputs.logits#replace verbalizer's gather_outputs() function\n        \n        if isinstance(outputs, tuple):\n            outputs_at_mask = [self.promptModel.extract_at_mask(output, batch) for output in outputs]\n        else:\n            outputs_at_mask = self.promptModel.extract_at_mask(outputs, batch)\n        \n        #use 5 verbalizers to replace original one\n        #label_words_logits = self.verbalizer.process_outputs(outputs_at_mask, batch=batch)\n        outputs_at_mask=torch.transpose(outputs_at_mask,0,1)\n        cpu_outputs_at_mask=outputs_at_mask[0]\n        #print('hjh check----->',cpu_outputs_at_mask.shape)\n        #cpu_outputs_at_mask=cpu_outputs_at_mask.view(cpu_outputs_at_mask.shape[0], cpu_outputs_at_mask.shape[2])\n        cpu_label_words_logits = self.cpu_promptVerbalizer.process_outputs(cpu_outputs_at_mask, batch=batch)\n        graphic_outputs_at_mask=outputs_at_mask[1]\n        #graphic_outputs_at_mask=graphic_outputs_at_mask.view(graphic_outputs_at_mask.shape[0], graphic_outputs_at_mask.shape[2])\n        graphic_label_words_logits = self.graphic_promptVerbalizer.process_outputs(graphic_outputs_at_mask, batch=batch)\n        hardisk_outputs_at_mask=outputs_at_mask[2]\n        #hardisk_outputs_at_mask=hardisk_outputs_at_mask.view(hardisk_outputs_at_mask.shape[0], hardisk_outputs_at_mask.shape[2])\n        hardisk_label_words_logits = self.hardisk_promptVerbalizer.process_outputs(hardisk_outputs_at_mask, batch=batch)\n        ram_outputs_at_mask=outputs_at_mask[3]\n        #ram_outputs_at_mask=ram_outputs_at_mask.view(ram_outputs_at_mask.shape[0], ram_outputs_at_mask.shape[2])\n        ram_label_words_logits = self.ram_promptVerbalizer.process_outputs(ram_outputs_at_mask, batch=batch)\n        screen_outputs_at_mask=outputs_at_mask[4]\n        #screen_outputs_at_mask=screen_outputs_at_mask.view(screen_outputs_at_mask.shape[0], screen_outputs_at_mask.shape[2])\n        screen_label_words_logits = self.screen_promptVerbalizer.process_outputs(screen_outputs_at_mask, batch=batch)\n        \n        return cpu_label_words_logits, graphic_label_words_logits, hardisk_label_words_logits,\\\n                ram_label_words_logits, screen_label_words_logits\n\n    def train(self):\n        self.promptModel.train()\n\n    def eval(self):\n        self.promptModel.eval()\n    \n    def set_epoch(self,epoch):\n        self.epoch=epoch\n\n\n\n\n\n\nif __name__ == '__main__':\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    print(device)\n\n    plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-cased\")\n\n    dataset=read_data_csv(\"/kaggle/input/ensembledata/model3_review_all_map.csv\",[])\n    \n    finetune_dataset=read_data_csv(\"/kaggle/input/ensembledata/model3_need_all_map.csv\",[])\n    test_dataset=read_data_csv(\"/kaggle/input/ensembledata/test_need_all_map.csv\",[])\n    need_dataset={}\n    need_dataset['train']=finetune_dataset['train']\n    need_dataset['valid']=test_dataset['train']\n    \n    epoch=10\n    \n    template='{\"soft\": \"Someone said : \"} {\"placeholder\":\"text_a\"} {\"soft\": \"So he need\"} a computer with a {\"mask\"} {\"mask\"} {\"mask\"} {\"mask\"} {\"mask\"} {\"soft\": \"configuration\"}'\n    \n    model=multiMask_MixTemplateModel(plm,\n                                   tokenizer,\n                                   WrapperClass,\n                                   dataset,\n                                   need_dataset,\n                                   epoch,\n                                   template,\n                                   device,\n                                   cpu_classes,\n                                    cpu_label_words,\n                                    graphic_classes,\n                                    graphic_label_words,\n                                    hard_classes,\n                                    hard_label_words,\n                                    ram_classes,\n                                    ram_label_words,\n                                    scre_classes,\n                                    scre_label_words)\n    \n    #-----------------------Train-------------------------\n    model.train()\n    for i in range(model.epoch):\n        count=0\n        loss_rec=0\n        for batch in model.train_data_loader:\n            batch.to(device)\n            \n            labels=batch['label']\n            label_trans=torch.transpose(batch['label'],0,1)\n            cpu_labels=label_trans[0]\n            graphic_labels=label_trans[1]\n            hard_labels=label_trans[2]\n            ram_labels=label_trans[3]\n            scre_labels=label_trans[4]\n            \n            #share model\n            cpu_logits, graphic_logits, hard_logits, ram_logits, scre_logits=model(batch)\n            \n            cpu_loss=model.cross_entropy(cpu_logits,cpu_labels)\n            graphic_loss=model.cross_entropy(graphic_logits,graphic_labels)\n            hard_loss=model.cross_entropy(hard_logits,hard_labels)\n            ram_loss=model.cross_entropy(ram_logits,ram_labels)\n            scre_loss=model.cross_entropy(scre_logits,scre_labels)\n            \n            shared_loss=cpu_loss+graphic_loss+hard_loss+ram_loss+scre_loss\n            \n            shared_loss.backward()\n            model.optimizer1.step()\n            model.optimizer1.zero_grad()\n            model.optimizer2.step()\n            model.optimizer2.zero_grad()\n            \n            count+=1\n            loss_rec+=shared_loss\n            \n        gc.collect()\n        torch.cuda.empty_cache()\n        print('NO.',i,' epoch avg loss: ',loss_rec/count)\n        \n    #-----------------------Fine tune-------------------------\n    model.train()\n    for i in range(20):\n        count=0\n        loss_rec=0\n        for batch in model.finetune_data_loader:\n            batch.to(device)\n            \n            labels=batch['label']\n            label_trans=torch.transpose(batch['label'],0,1)\n            cpu_labels=label_trans[0]\n            graphic_labels=label_trans[1]\n            hard_labels=label_trans[2]\n            ram_labels=label_trans[3]\n            scre_labels=label_trans[4]\n            \n            #share model\n            cpu_logits, graphic_logits, hard_logits, ram_logits, scre_logits=model(batch)\n            \n            cpu_loss=model.cross_entropy(cpu_logits,cpu_labels)\n            graphic_loss=model.cross_entropy(graphic_logits,graphic_labels)\n            hard_loss=model.cross_entropy(hard_logits,hard_labels)\n            ram_loss=model.cross_entropy(ram_logits,ram_labels)\n            scre_loss=model.cross_entropy(scre_logits,scre_labels)\n            \n            shared_loss=cpu_loss+graphic_loss+hard_loss+ram_loss+scre_loss\n            \n            shared_loss.backward()\n            model.optimizer1.step()\n            model.optimizer1.zero_grad()\n            model.optimizer2.step()\n            model.optimizer2.zero_grad()\n            \n            count+=1\n            loss_rec+=shared_loss\n            \n        gc.collect()\n        torch.cuda.empty_cache()\n        print('NO.',i,' epoch avg loss: ',loss_rec/count)\n        \n        #-----------------------test-------------------------\n        if(i==4 or i==9 or i==14 or i==19):\n            with torch.no_grad():\n                model.eval()\n                \n                #need_text=[]\n                \n                cpu_preds=[]\n                cpu_labels=[]\n                cpu_all_pred=[]\n                graphic_preds=[]\n                graphic_labels=[]\n                graphic_all_pred=[]\n                hard_preds=[]\n                hard_labels=[]\n                hard_all_pred=[]\n                ram_preds=[]\n                ram_labels=[]\n                ram_all_pred=[]\n                scre_preds=[]\n                scre_labels=[]\n                scre_all_pred=[]\n                \"\"\"all_pred=[]\n                preds=[]\n                labels=[]\"\"\"\n                for step, inputs in enumerate(model.test_data_loader):\n                    inputs.to(device)\n                    \"\"\"graphic_logits=model(inputs)\n                    graphic_label=inputs['label']\"\"\"\n                    #need_text.extend(inputs['input_ids'].cpu().tolist())\n                    \n                    cpu_logits, graphic_logits, hard_logits, ram_logits, scre_logits=model(inputs)\n\n                    label_trans=torch.transpose(inputs['label'],0,1)\n                    cpu_label=label_trans[0]\n                    graphic_label=label_trans[1]\n                    hard_label=label_trans[2]\n                    ram_label=label_trans[3]\n                    scre_label=label_trans[4]\n\n                    #need_text.extend(inputs['input_ids'].cpu().tolist())\n                    \n                    cpu_labels.extend(cpu_label.cpu().tolist())\n                    cpu_preds.extend(torch.argmax(cpu_logits,dim=-1).cpu().tolist())\n                    cpu_all_pred.extend(cpu_logits.cpu().tolist())\n                    graphic_labels.extend(graphic_label.cpu().tolist())\n                    graphic_preds.extend(torch.argmax(graphic_logits,dim=-1).cpu().tolist())\n                    graphic_all_pred.extend(graphic_logits.cpu().tolist())\n                    hard_labels.extend(hard_label.cpu().tolist())\n                    hard_preds.extend(torch.argmax(hard_logits,dim=-1).cpu().tolist())\n                    hard_all_pred.extend(hard_logits.cpu().tolist())\n                    ram_labels.extend(ram_label.cpu().tolist())\n                    ram_preds.extend(torch.argmax(ram_logits,dim=-1).cpu().tolist())\n                    ram_all_pred.extend(ram_logits.cpu().tolist())\n                    scre_labels.extend(scre_label.cpu().tolist())\n                    scre_preds.extend(torch.argmax(scre_logits,dim=-1).cpu().tolist())\n                    scre_all_pred.extend(scre_logits.cpu().tolist())\n                    \n                    \n                    \"\"\"all_pred.extend(graphic_logits.cpu().tolist())\n                    labels.extend(graphic_label.cpu().tolist())\n                    preds.extend(torch.argmax(graphic_logits,dim=-1).cpu().tolist())\"\"\"\n                cpu_acc=sum([int(i==j) for i,j in zip(cpu_preds, cpu_labels)])/len(cpu_preds)\n                graphic_acc=sum([int(i==j) for i,j in zip(graphic_preds, graphic_labels)])/len(graphic_preds)\n                hard_acc=sum([int(i==j) for i,j in zip(hard_preds, hard_labels)])/len(hard_preds)\n                ram_acc=sum([int(i==j) for i,j in zip(ram_preds, ram_labels)])/len(ram_preds)\n                scre_acc=sum([int(i==j) for i,j in zip(scre_preds, scre_labels)])/len(scre_preds)\n\n            save_path=\"/kaggle/working/ensemble1_model3_all_epoch_\"+str(i+1)+\"_test_res.csv\"\n            n=len(cpu_labels)\n            record=[]\n            for j in range(0,n):\n                tmp={\"index\":j, \\\n                                \"cpu_label\":cpu_labels[j], \"cpu_prediction\":cpu_preds[j], \"cpu_all_pred\": cpu_all_pred[j],\\\n                                \"graphic_label\":graphic_labels[j], \"graphic_prediction\":graphic_preds[j], \"graphic_all_pred\": graphic_all_pred[j],\\\n                                \"hard_label\":hard_labels[j], \"hard_prediction\":hard_preds[j], \"hard_all_pred\": hard_all_pred[j],\\\n                                \"ram_label\":ram_labels[j], \"ram_prediction\":ram_preds[j], \"ram_all_pred\": ram_all_pred[j],\\\n                                \"screen_label\":scre_labels[j], \"screen_prediction\":scre_preds[j], \"screen_all_pred\": scre_all_pred[j]}\n                record.append(tmp)\n\n            with open(save_path, 'w', newline='') as csvfile:\n                fieldnames = ['index', 'cpu_label','cpu_prediction','cpu_all_pred',\\\n                             'graphic_label','graphic_prediction','graphic_all_pred',\\\n                             'hard_label','hard_prediction','hard_all_pred',\\\n                             'ram_label','ram_prediction','ram_all_pred',\\\n                             'screen_label','screen_prediction','screen_all_pred']\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n                writer.writeheader()\n                writer.writerows(record)\n            print(save_path)\n            \"\"\"print(i,\" epoch test accuracy is : \",acc)\"\"\"\n            print(i,\" epoch test cpu accuracy is : \",cpu_acc)\n            print(i,\" epoch test graphic card accuracy is : \",graphic_acc)\n            print(i,\" epoch test hard disk accuracy is : \",hard_acc)\n            print(i,\" epoch test ram accuracy is : \",ram_acc)\n            print(i,\" epoch test scre accuracy is : \",scre_acc)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T08:26:56.008843Z","iopub.execute_input":"2023-07-04T08:26:56.009215Z","iopub.status.idle":"2023-07-04T08:58:33.802989Z","shell.execute_reply.started":"2023-07-04T08:26:56.009185Z","shell.execute_reply":"2023-07-04T08:58:33.801960Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c27c16b8d84145eba600129318f8bd49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56f43370915a4f2fb11052be486b04f5"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"079f0718520847488dc0eb84d9526dc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2ea55983661454ca7a5f0a8955c3b3b"}},"metadata":{}},{"name":"stderr","text":"tokenizing: 0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\ntokenizing: 2409it [00:09, 256.25it/s]\ntokenizing: 267it [00:00, 463.03it/s]\ntokenizing: 269it [00:00, 462.91it/s]\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"NO. 0  epoch avg loss:  tensor(14.8533, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 1  epoch avg loss:  tensor(14.2323, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 2  epoch avg loss:  tensor(13.7981, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 3  epoch avg loss:  tensor(13.3492, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 4  epoch avg loss:  tensor(12.8819, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 5  epoch avg loss:  tensor(12.3826, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 6  epoch avg loss:  tensor(11.8345, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 7  epoch avg loss:  tensor(11.3203, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 8  epoch avg loss:  tensor(10.9172, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 9  epoch avg loss:  tensor(10.4831, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 0  epoch avg loss:  tensor(15.1633, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 1  epoch avg loss:  tensor(13.8904, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 2  epoch avg loss:  tensor(13.5276, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 3  epoch avg loss:  tensor(13.3856, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 4  epoch avg loss:  tensor(13.1662, device='cuda:0', grad_fn=<DivBackward0>)\n/kaggle/working/ensemble1_model3_all_epoch_5_test_res.csv\n4  epoch test cpu accuracy is :  0.4684014869888476\n4  epoch test graphic card accuracy is :  0.30855018587360594\n4  epoch test hard disk accuracy is :  0.23048327137546468\n4  epoch test ram accuracy is :  0.6096654275092936\n4  epoch test scre accuracy is :  0.6877323420074349\nNO. 5  epoch avg loss:  tensor(12.5360, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 6  epoch avg loss:  tensor(11.9254, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 7  epoch avg loss:  tensor(11.2971, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 8  epoch avg loss:  tensor(10.6068, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 9  epoch avg loss:  tensor(9.9690, device='cuda:0', grad_fn=<DivBackward0>)\n/kaggle/working/ensemble1_model3_all_epoch_10_test_res.csv\n9  epoch test cpu accuracy is :  0.49814126394052044\n9  epoch test graphic card accuracy is :  0.5539033457249071\n9  epoch test hard disk accuracy is :  0.43866171003717475\n9  epoch test ram accuracy is :  0.620817843866171\n9  epoch test scre accuracy is :  0.6282527881040892\nNO. 10  epoch avg loss:  tensor(9.5449, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 11  epoch avg loss:  tensor(9.2636, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 12  epoch avg loss:  tensor(9.1426, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 13  epoch avg loss:  tensor(9.0397, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 14  epoch avg loss:  tensor(8.9645, device='cuda:0', grad_fn=<DivBackward0>)\n/kaggle/working/ensemble1_model3_all_epoch_15_test_res.csv\n14  epoch test cpu accuracy is :  0.4795539033457249\n14  epoch test graphic card accuracy is :  0.5576208178438662\n14  epoch test hard disk accuracy is :  0.5315985130111525\n14  epoch test ram accuracy is :  0.6133828996282528\n14  epoch test scre accuracy is :  0.6579925650557621\nNO. 15  epoch avg loss:  tensor(8.9113, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 16  epoch avg loss:  tensor(8.8856, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 17  epoch avg loss:  tensor(8.8590, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 18  epoch avg loss:  tensor(8.8468, device='cuda:0', grad_fn=<DivBackward0>)\nNO. 19  epoch avg loss:  tensor(8.8410, device='cuda:0', grad_fn=<DivBackward0>)\n/kaggle/working/ensemble1_model3_all_epoch_20_test_res.csv\n19  epoch test cpu accuracy is :  0.48698884758364314\n19  epoch test graphic card accuracy is :  0.5650557620817844\n19  epoch test hard disk accuracy is :  0.5464684014869888\n19  epoch test ram accuracy is :  0.6059479553903345\n19  epoch test scre accuracy is :  0.6468401486988847\n","output_type":"stream"}]},{"cell_type":"code","source":"tmp=[]\nfor step, inputs in enumerate(model.test_data_loader):\n    tmp.append(inputs['input_ids'].cpu().tolist())\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-04T06:49:15.073909Z","iopub.execute_input":"2023-07-04T06:49:15.074268Z","iopub.status.idle":"2023-07-04T06:49:15.089768Z","shell.execute_reply.started":"2023-07-04T06:49:15.074237Z","shell.execute_reply":"2023-07-04T06:49:15.088810Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"hjh_promptTemplate = MixedTemplate(\n            model=plm,\n            text = template,\n            tokenizer = tokenizer,\n        )\n\ntest_dataset1=read_data_csv(\"/kaggle/input/ensembledata/test_need_all_map.csv\",[])\ntest_loader1 = PromptDataLoader(\n            dataset = test_dataset1['train'],\n            tokenizer = tokenizer,\n            template = hjh_promptTemplate,\n            tokenizer_wrapper_class=WrapperClass,\n            batch_size=16,\n            #max_seq_length=800,\n        )\n\ntest_dataset2=read_data_csv(\"/kaggle/input/ensembledata/test_need_all_map.csv\",[])\ntest_loader2 = PromptDataLoader(\n            dataset = test_dataset2['train'],\n            tokenizer = tokenizer,\n            template = hjh_promptTemplate,\n            tokenizer_wrapper_class=WrapperClass,\n            batch_size=16,\n            #max_seq_length=800,\n        )\n\ntest_dataset3=read_data_csv(\"/kaggle/input/ensembledata/test_need_all_map.csv\",[])\ntest_loader3 = PromptDataLoader(\n            dataset = test_dataset3['train'],\n            tokenizer = tokenizer,\n            template = hjh_promptTemplate,\n            tokenizer_wrapper_class=WrapperClass,\n            batch_size=16,\n            #max_seq_length=800,\n        )","metadata":{"execution":{"iopub.status.busy":"2023-07-04T06:59:34.206747Z","iopub.execute_input":"2023-07-04T06:59:34.207120Z","iopub.status.idle":"2023-07-04T06:59:36.038275Z","shell.execute_reply.started":"2023-07-04T06:59:34.207088Z","shell.execute_reply":"2023-07-04T06:59:36.037191Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"tokenizing: 269it [00:00, 430.65it/s]\ntokenizing: 269it [00:00, 471.57it/s]\ntokenizing: 269it [00:00, 466.06it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"tmp1=[]\nfor item in test_loader1:\n    tmp1.append(item)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T07:00:50.851930Z","iopub.execute_input":"2023-07-04T07:00:50.852286Z","iopub.status.idle":"2023-07-04T07:00:50.866240Z","shell.execute_reply.started":"2023-07-04T07:00:50.852255Z","shell.execute_reply":"2023-07-04T07:00:50.865102Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"tmp2=[]\nfor item in test_loader2:\n    tmp2.append(item)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T07:00:58.074222Z","iopub.execute_input":"2023-07-04T07:00:58.074591Z","iopub.status.idle":"2023-07-04T07:00:58.086109Z","shell.execute_reply.started":"2023-07-04T07:00:58.074560Z","shell.execute_reply":"2023-07-04T07:00:58.085150Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(tmp1[11]['input_ids'][6])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T07:02:39.929934Z","iopub.execute_input":"2023-07-04T07:02:39.930290Z","iopub.status.idle":"2023-07-04T07:02:39.940477Z","shell.execute_reply.started":"2023-07-04T07:02:39.930261Z","shell.execute_reply":"2023-07-04T07:02:39.939429Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"tensor([  101,     0,     0,     0,   146,  1328,   170,  2775,  1114,   170,\n         1344,  2099,   118,  2616,  6022,   117,  1870,  1122,  1431,  1138,\n         7856, 12147,  1105,  1576,  4432,   119, 23122,  1103,  3076,  3068,\n         1110, 16865,   119,     0,     0,     0,   170,  2775,  1114,   170,\n          103,   103,   103,   103,   103,     0,   102,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tmp2[11]['input_ids'][6])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T07:02:44.770912Z","iopub.execute_input":"2023-07-04T07:02:44.771260Z","iopub.status.idle":"2023-07-04T07:02:44.784213Z","shell.execute_reply.started":"2023-07-04T07:02:44.771229Z","shell.execute_reply":"2023-07-04T07:02:44.783179Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"tensor([  101,     0,     0,     0,   146,  1328,   170,  2775,  1114,   170,\n         1344,  2099,   118,  2616,  6022,   117,  1870,  1122,  1431,  1138,\n         7856, 12147,  1105,  1576,  4432,   119, 23122,  1103,  3076,  3068,\n         1110, 16865,   119,     0,     0,     0,   170,  2775,  1114,   170,\n          103,   103,   103,   103,   103,     0,   102,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0])\n","output_type":"stream"}]},{"cell_type":"code","source":"print('1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}