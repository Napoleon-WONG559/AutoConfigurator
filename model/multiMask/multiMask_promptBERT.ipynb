{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-29T07:28:28.650639Z","iopub.status.busy":"2023-05-29T07:28:28.650062Z","iopub.status.idle":"2023-05-29T07:28:41.876101Z","shell.execute_reply":"2023-05-29T07:28:41.874885Z","shell.execute_reply.started":"2023-05-29T07:28:28.650569Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting openprompt\n","  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers>=4.10.0 in /opt/conda/lib/python3.10/site-packages (from openprompt) (4.29.2)\n","Collecting sentencepiece==0.1.96 (from openprompt)\n","  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.2 in /opt/conda/lib/python3.10/site-packages (from openprompt) (4.64.1)\n","Requirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (from openprompt) (2.6)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from openprompt) (3.2.4)\n","Collecting yacs (from openprompt)\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from openprompt) (0.3.6)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from openprompt) (2.1.0)\n","Collecting rouge==1.0.0 (from openprompt)\n","  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from openprompt) (10.0.1)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from openprompt) (1.10.1)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (5.4.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (2023.5.5)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (2.28.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.10.0->openprompt) (0.13.3)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->openprompt) (1.5.3)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->openprompt) (3.2.0)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->openprompt) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets->openprompt) (2023.5.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->openprompt) (3.8.4)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->openprompt) (0.18.0)\n","Requirement already satisfied: protobuf<4,>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from tensorboardX->openprompt) (3.20.3)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (2.1.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (1.9.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->openprompt) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.10.0->openprompt) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.10.0->openprompt) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.10.0->openprompt) (2023.5.7)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->openprompt) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->openprompt) (2023.3)\n","Installing collected packages: sentencepiece, yacs, rouge, openprompt\n","  Attempting uninstall: sentencepiece\n","    Found existing installation: sentencepiece 0.1.99\n","    Uninstalling sentencepiece-0.1.99:\n","      Successfully uninstalled sentencepiece-0.1.99\n","Successfully installed openprompt-1.0.1 rouge-1.0.0 sentencepiece-0.1.96 yacs-0.1.8\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install openprompt"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-29T07:28:47.119962Z","iopub.status.busy":"2023-05-29T07:28:47.119604Z","iopub.status.idle":"2023-05-29T08:48:19.738239Z","shell.execute_reply":"2023-05-29T08:48:19.737098Z","shell.execute_reply.started":"2023-05-29T07:28:47.119931Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"]},{"name":"stdout","output_type":"stream","text":["cuda:0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97761cbe0d504b778a03a8609808017b","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4c2aeb227174063926ea6dcfa52fe8d","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70c16df04c4748f7bcef0eb92f950a85","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7af98d537f143828e31791f57c39cf7","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["tokenizing: 0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (786 > 512). Running this sequence through the model will result in indexing errors\n","tokenizing: 6504it [00:24, 263.69it/s]\n","tokenizing: 723it [00:03, 238.97it/s]\n","tokenizing: 535it [00:01, 477.02it/s]\n","tokenizing: 535it [00:01, 484.61it/s]\n","/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["NO. 0  epoch avg loss:  tensor(14.4431, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 1  epoch avg loss:  tensor(13.6099, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 2  epoch avg loss:  tensor(13.1019, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 3  epoch avg loss:  tensor(12.6277, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 4  epoch avg loss:  tensor(12.1491, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 5  epoch avg loss:  tensor(11.6620, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 6  epoch avg loss:  tensor(11.2402, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 7  epoch avg loss:  tensor(10.8739, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 8  epoch avg loss:  tensor(10.5372, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 9  epoch avg loss:  tensor(10.2314, device='cuda:0', grad_fn=<DivBackward0>)\n","cpu accuracy is :  0.42323651452282157\n","graphic card accuracy is :  0.4107883817427386\n","hard disk accuracy is :  0.5186721991701245\n","ram accuracy is :  0.6445366528354081\n","scre accuracy is :  0.5670816044260027\n","NO. 0  epoch avg loss:  tensor(14.3024, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 1  epoch avg loss:  tensor(13.6248, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 2  epoch avg loss:  tensor(13.3632, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 3  epoch avg loss:  tensor(13.1055, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 4  epoch avg loss:  tensor(12.8731, device='cuda:0', grad_fn=<DivBackward0>)\n","/kaggle/working/mixprompt_all_epoch_5_test_res.csv\n","4  epoch test cpu accuracy is :  0.4654205607476635\n","4  epoch test graphic card accuracy is :  0.27289719626168224\n","4  epoch test hard disk accuracy is :  0.35887850467289717\n","4  epoch test ram accuracy is :  0.5644859813084112\n","4  epoch test scre accuracy is :  0.7271028037383177\n","NO. 5  epoch avg loss:  tensor(12.2246, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 6  epoch avg loss:  tensor(11.4793, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 7  epoch avg loss:  tensor(10.6608, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 8  epoch avg loss:  tensor(10.0975, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 9  epoch avg loss:  tensor(9.6692, device='cuda:0', grad_fn=<DivBackward0>)\n","/kaggle/working/mixprompt_all_epoch_10_test_res.csv\n","9  epoch test cpu accuracy is :  0.4785046728971963\n","9  epoch test graphic card accuracy is :  0.4523364485981308\n","9  epoch test hard disk accuracy is :  0.4280373831775701\n","9  epoch test ram accuracy is :  0.6299065420560748\n","9  epoch test scre accuracy is :  0.6915887850467289\n","NO. 10  epoch avg loss:  tensor(9.4382, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 11  epoch avg loss:  tensor(9.2756, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 12  epoch avg loss:  tensor(9.1493, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 13  epoch avg loss:  tensor(9.0947, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 14  epoch avg loss:  tensor(9.0644, device='cuda:0', grad_fn=<DivBackward0>)\n","/kaggle/working/mixprompt_all_epoch_15_test_res.csv\n","14  epoch test cpu accuracy is :  0.4747663551401869\n","14  epoch test graphic card accuracy is :  0.502803738317757\n","14  epoch test hard disk accuracy is :  0.5570093457943925\n","14  epoch test ram accuracy is :  0.6186915887850467\n","14  epoch test scre accuracy is :  0.708411214953271\n","NO. 15  epoch avg loss:  tensor(9.0492, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 16  epoch avg loss:  tensor(9.0146, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 17  epoch avg loss:  tensor(8.9924, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 18  epoch avg loss:  tensor(8.9900, device='cuda:0', grad_fn=<DivBackward0>)\n","NO. 19  epoch avg loss:  tensor(8.9875, device='cuda:0', grad_fn=<DivBackward0>)\n","/kaggle/working/mixprompt_all_epoch_20_test_res.csv\n","19  epoch test cpu accuracy is :  0.4803738317757009\n","19  epoch test graphic card accuracy is :  0.5233644859813084\n","19  epoch test hard disk accuracy is :  0.5457943925233645\n","19  epoch test ram accuracy is :  0.6280373831775701\n","19  epoch test scre accuracy is :  0.6990654205607477\n"]}],"source":["import torch\n","from openprompt.data_utils import InputExample\n","import csv\n","from openprompt.plms import load_plm\n","from openprompt.prompts import MixedTemplate\n","from transformers.utils.dummy_pt_objects import PreTrainedModel\n","from openprompt.prompts import ManualVerbalizer\n","from openprompt import PromptForClassification\n","from openprompt import PromptDataLoader\n","from transformers.tokenization_utils import PreTrainedTokenizer\n","import torch.nn as nn\n","from transformers import AdamW\n","from torch.utils.data import random_split\n","from openprompt.prompts import ManualTemplate\n","import gc\n","\n","#graphic card classes and label words\n","graphic_classes = [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4\n","]\n","graphic_label_words={\n","    0:['NVIDIA GeForce GTX 1050', 'NVIDIA GeForce GTX 1050 Ti', 'NVIDIA GeForce GTX 1060', 'NVIDIA GeForce GTX 1070', 'NVIDIA GeForce 940MX'],\n","    1:['AMD Radeon R2', 'AMD Radeon R4', 'AMD Radeon R5 Graphics', 'AMD Radeon R7'],\n","    2:['Intel UHD Graphics 620', 'Intel Iris Plus Graphics 640', 'Intel HD Graphics 3000', 'Intel', 'Intel HD 620 graphics', 'Intel HD Graphics 500', 'Intel HD Graphics 520', 'Intel HD Graphics 620', 'Intel HD Graphics 400', 'Intel Celeron', 'Intel HD Graphics 505', 'Intel HD Graphics 5500', 'Intel HD Graphics', 'Intel?? HD Graphics 620 (up to 2.07 GB)', 'intel 620'],\n","    3:['Integrated', 'integrated intel hd graphics', 'integrated AMD Radeon R5 Graphics', 'Integrated Graphics', 'Integrated intel hd graphics'],\n","    4:['others'],\n","}\n","cpu_classes = [\n","    0,\n","    1,\n","    2,\n","    3\n","]\n","cpu_label_words = {\n","    0: ['4 GHz Intel Core i7'],\n","    1: ['3.8 GHz Intel Core i7', '3.8 GHz Core i7 Family', '3.5 GHz Intel Core i7', '3 GHz 8032', '3.5 GHz 8032', '3 GHz AMD A Series', '3.1 GHz Intel Core i5', '3.4 GHz Intel Core i5', '3.6 GHz AMD A Series', '3.5 GHz Intel Core i5', '3 GHz'],\n","    2: ['2.8 GHz Intel Core i7', '2.7 GHz Core i7 7500U', '2.7 GHz Core i7 2.7 GHz', '2.7 GHz Intel Core i7', '2.1 GHz Intel Core i7', '2.2 GHz Intel Core i5', '2.3 GHz Intel Core i5', '2.6 GHz Intel Core i5', '2.5 GHz Intel Core i5', '2.5 GHz Core i5 7200U', '2 GHz None', '2 GHz AMD A Series', '2.7 GHz Intel Core i3', '2.5 GHz Pentium', '2.5 GHz AMD A Series', '2.16 GHz Intel Celeron', '2.16 GHz Athlon 2650e', '2.7 GHz 8032', '2.48 GHz Intel Celeron', '2.4 GHz AMD A Series', '2 GHz Celeron D Processor 360', '2.4 GHz Intel Core i3', '2.3 GHz Intel Core i3', '2.4 GHz Core i3-540', '2.5 GHz Intel Core Duo', '2.2 GHz Intel Core i3', '2.7 GHz AMD A Series', '2.8 GHz 8032', '2.5 GHz Athlon 2650e', '2.9 GHz Intel Celeron', '2 GB'],\n","    3: ['1.5 GHz', '1.8 GHz 8032', '1.8 GHz AMD E Series', '1.7 GHz', '1.1 GHz Intel Celeron', '1.6 GHz Intel Celeron', '1.6 GHz Intel Core 2 Duo', '1.7 GHz Exynos 5000 Series', '1.6 GHz Celeron N3060', '1.6 GHz AMD E Series', '1.1 GHz Pentium', '1.6 GHz', '1.6 GHz Intel Mobile CPU', '1.6 GHz Celeron N3050', '1.8 GHz Intel Core i7', '1.6 GHz Intel Core i5'],\n","}\n","hard_classes = [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5\n","]\n","hard_label_words = {\n","    0: ['2 TB HDD 5400 rpm'],\n","    1: ['1 TB', '1 TB HDD 7200 rpm', '1000 GB Mechanical Hard Drive', '1000 GB Hybrid Drive', '1 TB HDD 5400 rpm', '1024 GB Mechanical Hard Drive', '1 TB serial_ata', '1 TB mechanical_hard_drive', '1128 GB Hybrid'],\n","    2: ['500 GB HDD 5400 rpm', '500 GB mechanical_hard_drive', 'Solid State Drive, 512 GB', '512 GB SSD'],\n","    3: ['256 GB Flash Memory Solid State', '256 GB', '256.00 SSD', '256 GB SSD', '320 GB HDD 5400 rpm'],\n","    4: ['128 GB Flash Memory Solid State', '128 GB SSD'],\n","    5: ['others'],\n","}\n","ram_classes = [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5,\n","    6\n","]\n","ram_label_words = {\n","    0: ['16 GB DDR4', '16 GB LPDDR3_SDRAM', '16 GB SDRAM', '16 GB DDR SDRAM'],\n","    1: ['12 GB', '12 GB DDR3', '12 GB DDR SDRAM'],\n","    2: ['8 GB SDRAM DDR3', '8 GB DDR3 SDRAM', '8 GB DDR4 2666MHz', '8 GB DDR4', '8 GB LPDDR3', '8 GB DDR4 SDRAM', '8 GB DDR4_SDRAM', '8 GB 2-in1 Media Card Reader, USB 3.1, Type-C', '8 GB DDR SDRAM', '8 GB SDRAM DDR4', '8 GB ddr4', '8 GB sdram', '8 GB SDRAM', '8 GB'],\n","    3: ['6 GB SDRAM', '6 GB', '6 GB SDRAM DDR4', '6 GB DDR SDRAM'],\n","    4: ['4 GB LPDDR3_SDRAM', '4 GB SDRAM DDR4', '4 GB ddr3_sdram', '4 GB DDR3', '4 GB SDRAM', '4 GB', '4 GB SDRAM DDR3', '4 GB DDR4', '4 GB DDR3 SDRAM', '4 GB DDR SDRAM'],\n","    5: ['2 GB SDRAM DDR3', '2 GB SDRAM', '2 GB DDR3L SDRAM', '2 GB DDR3 SDRAM'],\n","    6: ['others'],\n","}\n","scre_classes = [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5,\n","    6,\n","    7\n","]\n","scre_label_words = {\n","    0: ['19.5 inches'],\n","    1: ['17.3 inches'],\n","    2: ['15.6 inches'],\n","    3: ['14 inches'],\n","    4: ['13.5 inches', '13.3 inches'],\n","    5: ['12.5 inches', '12.3 inches'],\n","    6: ['11.6 inches'],\n","    7: ['10.1 inches'],\n","}\n","\n","def copy_batch(batch,device):\n","    n_batch={}\n","    for key in batch.keys():\n","        ni=torch.tensor(batch[key],device=device)\n","        n_batch[key]=ni\n","    return n_batch\n","        \n","\n","def read_data_csv(file,ratio):\n","    record=[]\n","    with open(file,newline='') as csvfile:\n","        read=csv.reader(csvfile)\n","        for item in read:\n","            record.append(item[1:])\n","    record=record[1:]\n","    for ind,sample in enumerate(record):\n","        sample.insert(0,ind)\n","        sample[2]=int(sample[2])#cpu\n","        sample[3]=int(sample[3])#graphic\n","        sample[4]=int(sample[4])#hardisk\n","        sample[5]=int(sample[5])#ram\n","        sample[6]=int(sample[6])#screen\n","    train_set, valid_set=random_split(record,\n","                 #[0.7,0.3],\n","                 ratio,\n","                 generator=torch.Generator().manual_seed(42))\n","    dataset={}\n","    train_dataset=[]\n","    valid_dataset=[]\n","    for item in train_set:\n","        train_dataset.append(InputExample(guid=item[0],text_a=item[1],label=item[2:]))\n","    for item in valid_set:\n","        valid_dataset.append(InputExample(guid=item[0],text_a=item[1],label=item[2:]))\n","    dataset['train']=train_dataset\n","    dataset['valid']=valid_dataset\n","    return dataset\n","\n","\n","\n","\n","class multiMask_MixTemplateModel(nn.Module):\n","    def __init__(self,\n","                plm:PreTrainedModel,\n","                tokenizer: PreTrainedTokenizer,\n","                WrapperClass,\n","                dataset,\n","                needdata,\n","                #classes,\n","                epoch,\n","                template_text,\n","                #shareTemplate,\n","                #label_words,\n","                device,\n","                cpu_classes,\n","                cpu_label_words,\n","                graphic_classes,\n","                grapihc_label_words,\n","                hardisk_classes,\n","                hardisk_label_words,\n","                ram_classes,\n","                ram_label_words,\n","                screen_classes,\n","                screen_label_words,\n","                ):\n","        \n","        super().__init__()\n","        \n","        #self.promptTemplate = shareTemplate\n","        self.promptTemplate = MixedTemplate(\n","            model=plm,\n","            text = template_text,\n","            tokenizer = tokenizer,\n","        )\n","\n","        # 5 verbalizer correpond to 5 attributes(cpu, graphic card, hard disk, ram, screen)\n","        self.cpu_promptVerbalizer = ManualVerbalizer(\n","            classes = cpu_classes,\n","            label_words = cpu_label_words,\n","            tokenizer = tokenizer,\n","            multi_token_handler=\"first\",\n","        )\n","        self.cpu_promptVerbalizer.to(device)\n","        self.graphic_promptVerbalizer = ManualVerbalizer(\n","            classes = graphic_classes,\n","            label_words = graphic_label_words,\n","            tokenizer = tokenizer,\n","            multi_token_handler=\"first\",\n","        )\n","        self.graphic_promptVerbalizer.to(device)\n","        self.hardisk_promptVerbalizer = ManualVerbalizer(\n","            classes = hardisk_classes,\n","            label_words = hardisk_label_words,\n","            tokenizer = tokenizer,\n","            multi_token_handler=\"first\",\n","        )\n","        self.hardisk_promptVerbalizer.to(device)\n","        self.ram_promptVerbalizer = ManualVerbalizer(\n","            classes = ram_classes,\n","            label_words = ram_label_words,\n","            tokenizer = tokenizer,\n","            multi_token_handler=\"first\",\n","        )\n","        self.ram_promptVerbalizer.to(device)\n","        self.screen_promptVerbalizer = ManualVerbalizer(\n","            classes = screen_classes,\n","            label_words = screen_label_words,\n","            tokenizer = tokenizer,\n","            multi_token_handler=\"first\",\n","        )\n","        self.screen_promptVerbalizer.to(device)\n","\n","        # Model backbone\n","        self.promptModel = PromptForClassification(\n","            template = self.promptTemplate,\n","            plm = plm,\n","            #verbalizer = self.promptVerbalizer,\n","            verbalizer = None,#rewrite model foward function and use the verbalizer outside\n","        )\n","        self.promptModel.to(device)\n","\n","        #train_set, valid_set=random_split(dataset,\n","        #                                  [0.7,0.3],\n","        #                                  generator=torch.Generator().manual_seed(42))\n","        train_set=dataset['train']\n","        valid_set=dataset['valid']\n","        \n","        finetune_set=needdata['train']\n","        test_set=needdata['valid']\n","\n","        self.train_data_loader = PromptDataLoader(\n","            dataset = train_set,\n","            tokenizer = tokenizer,\n","            template = self.promptTemplate,\n","            tokenizer_wrapper_class=WrapperClass,\n","            batch_size=16,\n","            shuffle=True,\n","            #max_seq_length=800,\n","        )\n","        self.valid_data_loader = PromptDataLoader(\n","            dataset = valid_set,\n","            tokenizer = tokenizer,\n","            template = self.promptTemplate,\n","            tokenizer_wrapper_class=WrapperClass,\n","            batch_size=16,\n","            #max_seq_length=800,\n","        )\n","        \n","        self.finetune_data_loader = PromptDataLoader(\n","            dataset = finetune_set,\n","            tokenizer = tokenizer,\n","            template = self.promptTemplate,\n","            tokenizer_wrapper_class=WrapperClass,\n","            batch_size=16,\n","            shuffle=True,\n","            #max_seq_length=800,\n","        )\n","        self.test_data_loader = PromptDataLoader(\n","            dataset = test_set,\n","            tokenizer = tokenizer,\n","            template = self.promptTemplate,\n","            tokenizer_wrapper_class=WrapperClass,\n","            batch_size=16,\n","            #max_seq_length=800,\n","        )\n","\n","        self.cross_entropy  = nn.NLLLoss()\n","        no_decay = ['bias', 'LayerNorm.weight']\n","        # it's always good practice to set no decay to biase and LayerNorm parameters\n","        optimizer_grouped_parameters1 = [\n","            {'params': [p for n, p in self.promptModel.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","            {'params': [p for n, p in self.promptModel.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","        # Using different optimizer for prompt parameters and model parameters\n","        optimizer_grouped_parameters2 = [\n","            {'params': [p for n,p in self.promptModel.template.named_parameters() if \"raw_embedding\" not in n]}\n","        ]\n","        self.optimizer1 = AdamW(optimizer_grouped_parameters1, lr=1e-5)\n","        self.optimizer2 = AdamW(optimizer_grouped_parameters2, lr=1e-5)\n","\n","        self.epoch=epoch\n","\n","    def forward(self,batch):\n","        \"\"\"outputs=self.promptModel(batch)\"\"\"\n","        #rewrite forward function\n","        \n","        outputs = self.promptModel.prompt_model(batch)\n","        \n","        #outputs = self.verbalizer.gather_outputs(outputs)\n","        outputs=outputs.logits#replace verbalizer's gather_outputs() function\n","        \n","        if isinstance(outputs, tuple):\n","            outputs_at_mask = [self.promptModel.extract_at_mask(output, batch) for output in outputs]\n","        else:\n","            outputs_at_mask = self.promptModel.extract_at_mask(outputs, batch)\n","        \n","        #use 5 verbalizers to replace original one\n","        #label_words_logits = self.verbalizer.process_outputs(outputs_at_mask, batch=batch)\n","        outputs_at_mask=torch.transpose(outputs_at_mask,0,1)\n","        cpu_outputs_at_mask=outputs_at_mask[0]\n","        #print('hjh check----->',cpu_outputs_at_mask.shape)\n","        #cpu_outputs_at_mask=cpu_outputs_at_mask.view(cpu_outputs_at_mask.shape[0], cpu_outputs_at_mask.shape[2])\n","        cpu_label_words_logits = self.cpu_promptVerbalizer.process_outputs(cpu_outputs_at_mask, batch=batch)\n","        graphic_outputs_at_mask=outputs_at_mask[1]\n","        #graphic_outputs_at_mask=graphic_outputs_at_mask.view(graphic_outputs_at_mask.shape[0], graphic_outputs_at_mask.shape[2])\n","        graphic_label_words_logits = self.graphic_promptVerbalizer.process_outputs(graphic_outputs_at_mask, batch=batch)\n","        hardisk_outputs_at_mask=outputs_at_mask[2]\n","        #hardisk_outputs_at_mask=hardisk_outputs_at_mask.view(hardisk_outputs_at_mask.shape[0], hardisk_outputs_at_mask.shape[2])\n","        hardisk_label_words_logits = self.hardisk_promptVerbalizer.process_outputs(hardisk_outputs_at_mask, batch=batch)\n","        ram_outputs_at_mask=outputs_at_mask[3]\n","        #ram_outputs_at_mask=ram_outputs_at_mask.view(ram_outputs_at_mask.shape[0], ram_outputs_at_mask.shape[2])\n","        ram_label_words_logits = self.ram_promptVerbalizer.process_outputs(ram_outputs_at_mask, batch=batch)\n","        screen_outputs_at_mask=outputs_at_mask[4]\n","        #screen_outputs_at_mask=screen_outputs_at_mask.view(screen_outputs_at_mask.shape[0], screen_outputs_at_mask.shape[2])\n","        screen_label_words_logits = self.screen_promptVerbalizer.process_outputs(screen_outputs_at_mask, batch=batch)\n","        \n","        return cpu_label_words_logits, graphic_label_words_logits, hardisk_label_words_logits,\\\n","                ram_label_words_logits, screen_label_words_logits\n","\n","    def train(self):\n","        self.promptModel.train()\n","\n","    def eval(self):\n","        self.promptModel.eval()\n","    \n","    def set_epoch(self,epoch):\n","        self.epoch=epoch\n","\n","\n","\n","\n","\n","\n","if __name__ == '__main__':\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    print(device)\n","\n","    #load pre-trained model\n","    #graphic_plm, graphic_tokenizer, graphic_model_config, graphic_WrapperClass = load_plm(\"bert\", \"bert-base-cased\")\n","    plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-cased\")\n","\n","    #graphic model\n","    #graphic_dataset=read_data_csv(\"/kaggle/input/newreviewallmap/review_all_map.csv\")\n","    dataset=read_data_csv(\"/kaggle/input/reviewallmap/review_all_map.csv\",[6504,723])\n","    need_dataset=read_data_csv(\"/kaggle/input/needallmap/need_all_map.csv\",[535,535])\n","    #graphic_train_set, graphic_valid_set=random_split(graphic_dataset,\n","    #                                                  [0.7,0.3],\n","    #                                                  generator=torch.Generator().manual_seed(42))\n","    #graphic_epoch=5\n","    epoch=10\n","    #graphic_template='{\"soft\": \"Someone want to buy a computer and he said that\"} {\"placeholder\":\"text_a\"} So he want {\"mask\"}'\n","    #template='{\"soft\": \"Someone want to buy a computer and he said that\"} {\"placeholder\":\"text_a\"} So he want {\"mask\"}'\n","    #graphic_template='{\"placeholder\":\"text_a\"} {\"mask\"}'\n","    template='{\"soft\": \"Someone said : \"} {\"placeholder\":\"text_a\"} {\"soft\": \"So he need\"} a computer with a {\"mask\"} {\"mask\"} {\"mask\"} {\"mask\"} {\"mask\"} {\"soft\": \"configuration\"}'\n","    \"\"\"shareTemplate = MixedTemplate(\n","        model=plm,\n","        text = template,\n","        tokenizer = tokenizer,\n","    )\"\"\"\n","    model=multiMask_MixTemplateModel(plm,\n","                                   tokenizer,\n","                                   WrapperClass,\n","                                   dataset,\n","                                   need_dataset,\n","                                   #graphic_classes,\n","                                   epoch,\n","                                   #shareTemplate,\n","                                   template,\n","                                   #graphic_label_words,\n","                                   device,\n","                                   cpu_classes,\n","                                    cpu_label_words,\n","                                    graphic_classes,\n","                                    graphic_label_words,\n","                                    hard_classes,\n","                                    hard_label_words,\n","                                    ram_classes,\n","                                    ram_label_words,\n","                                    scre_classes,\n","                                    scre_label_words)\n","    \n","\n","    \n","    #-----------------------Train-------------------------\n","    model.train()\n","    for i in range(model.epoch):\n","        count=0\n","        loss_rec=0\n","        for batch in model.train_data_loader:\n","            batch.to(device)\n","            \n","            labels=batch['label']\n","            label_trans=torch.transpose(batch['label'],0,1)\n","            cpu_labels=label_trans[0]\n","            graphic_labels=label_trans[1]\n","            hard_labels=label_trans[2]\n","            ram_labels=label_trans[3]\n","            scre_labels=label_trans[4]\n","            \n","            #share model\n","            cpu_logits, graphic_logits, hard_logits, ram_logits, scre_logits=model(batch)\n","            \n","            cpu_loss=model.cross_entropy(cpu_logits,cpu_labels)\n","            graphic_loss=model.cross_entropy(graphic_logits,graphic_labels)\n","            hard_loss=model.cross_entropy(hard_logits,hard_labels)\n","            ram_loss=model.cross_entropy(ram_logits,ram_labels)\n","            scre_loss=model.cross_entropy(scre_logits,scre_labels)\n","            \n","            shared_loss=cpu_loss+graphic_loss+hard_loss+ram_loss+scre_loss\n","            \n","            shared_loss.backward()\n","            model.optimizer1.step()\n","            model.optimizer1.zero_grad()\n","            model.optimizer2.step()\n","            model.optimizer2.zero_grad()\n","            \n","            count+=1\n","            loss_rec+=shared_loss\n","            \n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        print('NO.',i,' epoch avg loss: ',loss_rec/count)\n","    \n","    #save checkpoint\n","    \"\"\"PATH = \"/kaggle/working/PTMTLtemplateEmb2.pt\"\n","    torch.save({\n","            'epoch': graphic_model.epoch,\n","            'model_state_dict': graphic_model.promptModel.template.state_dict(),\n","            }, PATH)\"\"\"\n","    \n","    #-----------------------Validate-------------------------\n","    model.eval()\n","    cpu_preds=[]\n","    cpu_labels=[]\n","    graphic_preds=[]\n","    graphic_labels=[]\n","    hard_preds=[]\n","    hard_labels=[]\n","    ram_preds=[]\n","    ram_labels=[]\n","    scre_preds=[]\n","    scre_labels=[]\n","    with torch.no_grad():\n","        #for step, inputs in enumerate(graphic_model.valid_data_loader):\n","        for step, inputs in enumerate(model.valid_data_loader):\n","            inputs.to(device)\n","\n","            #share model\n","            cpu_logits, graphic_logits, hard_logits, ram_logits, scre_logits=model(inputs)\n","\n","            #graphic_logits=graphic_model(inputs)\n","            #all_label=inputs['label']\n","            label_trans=torch.transpose(inputs['label'],0,1)\n","            cpu_label=label_trans[0]\n","            graphic_label=label_trans[1]\n","            hard_label=label_trans[2]\n","            ram_label=label_trans[3]\n","            scre_label=label_trans[4]\n","\n","            cpu_labels.extend(cpu_label.cpu().tolist())\n","            cpu_preds.extend(torch.argmax(cpu_logits,dim=-1).cpu().tolist())\n","            graphic_labels.extend(graphic_label.cpu().tolist())\n","            graphic_preds.extend(torch.argmax(graphic_logits,dim=-1).cpu().tolist())\n","            hard_labels.extend(hard_label.cpu().tolist())\n","            hard_preds.extend(torch.argmax(hard_logits,dim=-1).cpu().tolist())\n","            ram_labels.extend(ram_label.cpu().tolist())\n","            ram_preds.extend(torch.argmax(ram_logits,dim=-1).cpu().tolist())\n","            scre_labels.extend(scre_label.cpu().tolist())\n","            scre_preds.extend(torch.argmax(scre_logits,dim=-1).cpu().tolist())\n","        \n","    cpu_acc=sum([int(i==j) for i,j in zip(cpu_preds, cpu_labels)])/len(cpu_preds)\n","    graphic_acc=sum([int(i==j) for i,j in zip(graphic_preds, graphic_labels)])/len(graphic_preds)\n","    hard_acc=sum([int(i==j) for i,j in zip(hard_preds, hard_labels)])/len(hard_preds)\n","    ram_acc=sum([int(i==j) for i,j in zip(ram_preds, ram_labels)])/len(ram_preds)\n","    scre_acc=sum([int(i==j) for i,j in zip(scre_preds, scre_labels)])/len(scre_preds)\n","\n","    print(\"cpu accuracy is : \",cpu_acc)\n","    print(\"graphic card accuracy is : \",graphic_acc)\n","    print(\"hard disk accuracy is : \",hard_acc)\n","    print(\"ram accuracy is : \",ram_acc)\n","    print(\"scre accuracy is : \",scre_acc)\n","\n","    \n","    #-----------------------Fine tune-------------------------\n","    model.train()\n","    for i in range(20):\n","        count=0\n","        loss_rec=0\n","        for batch in model.finetune_data_loader:\n","            batch.to(device)\n","            \n","            labels=batch['label']\n","            label_trans=torch.transpose(batch['label'],0,1)\n","            cpu_labels=label_trans[0]\n","            graphic_labels=label_trans[1]\n","            hard_labels=label_trans[2]\n","            ram_labels=label_trans[3]\n","            scre_labels=label_trans[4]\n","            \n","            #share model\n","            cpu_logits, graphic_logits, hard_logits, ram_logits, scre_logits=model(batch)\n","            \n","            cpu_loss=model.cross_entropy(cpu_logits,cpu_labels)\n","            graphic_loss=model.cross_entropy(graphic_logits,graphic_labels)\n","            hard_loss=model.cross_entropy(hard_logits,hard_labels)\n","            ram_loss=model.cross_entropy(ram_logits,ram_labels)\n","            scre_loss=model.cross_entropy(scre_logits,scre_labels)\n","            \n","            shared_loss=cpu_loss+graphic_loss+hard_loss+ram_loss+scre_loss\n","            \n","            shared_loss.backward()\n","            model.optimizer1.step()\n","            model.optimizer1.zero_grad()\n","            model.optimizer2.step()\n","            model.optimizer2.zero_grad()\n","            \n","            count+=1\n","            loss_rec+=shared_loss\n","            \n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        print('NO.',i,' epoch avg loss: ',loss_rec/count)\n","        \n","        #-----------------------test-------------------------\n","        if(i==4 or i==9 or i==14 or i==19):\n","            with torch.no_grad():\n","                model.eval()\n","                cpu_preds=[]\n","                cpu_labels=[]\n","                cpu_all_pred=[]\n","                graphic_preds=[]\n","                graphic_labels=[]\n","                graphic_all_pred=[]\n","                hard_preds=[]\n","                hard_labels=[]\n","                hard_all_pred=[]\n","                ram_preds=[]\n","                ram_labels=[]\n","                ram_all_pred=[]\n","                scre_preds=[]\n","                scre_labels=[]\n","                scre_all_pred=[]\n","                \"\"\"all_pred=[]\n","                preds=[]\n","                labels=[]\"\"\"\n","                for step, inputs in enumerate(model.test_data_loader):\n","                    inputs.to(device)\n","                    \"\"\"graphic_logits=model(inputs)\n","                    graphic_label=inputs['label']\"\"\"\n","                    cpu_logits, graphic_logits, hard_logits, ram_logits, scre_logits=model(inputs)\n","\n","                    label_trans=torch.transpose(inputs['label'],0,1)\n","                    cpu_label=label_trans[0]\n","                    graphic_label=label_trans[1]\n","                    hard_label=label_trans[2]\n","                    ram_label=label_trans[3]\n","                    scre_label=label_trans[4]\n","\n","                    cpu_labels.extend(cpu_label.cpu().tolist())\n","                    cpu_preds.extend(torch.argmax(cpu_logits,dim=-1).cpu().tolist())\n","                    cpu_all_pred.extend(cpu_logits.cpu().tolist())\n","                    graphic_labels.extend(graphic_label.cpu().tolist())\n","                    graphic_preds.extend(torch.argmax(graphic_logits,dim=-1).cpu().tolist())\n","                    graphic_all_pred.extend(graphic_logits.cpu().tolist())\n","                    hard_labels.extend(hard_label.cpu().tolist())\n","                    hard_preds.extend(torch.argmax(hard_logits,dim=-1).cpu().tolist())\n","                    hard_all_pred.extend(hard_logits.cpu().tolist())\n","                    ram_labels.extend(ram_label.cpu().tolist())\n","                    ram_preds.extend(torch.argmax(ram_logits,dim=-1).cpu().tolist())\n","                    ram_all_pred.extend(ram_logits.cpu().tolist())\n","                    scre_labels.extend(scre_label.cpu().tolist())\n","                    scre_preds.extend(torch.argmax(scre_logits,dim=-1).cpu().tolist())\n","                    scre_all_pred.extend(scre_logits.cpu().tolist())\n","                    \n","                    \n","                    \"\"\"all_pred.extend(graphic_logits.cpu().tolist())\n","                    labels.extend(graphic_label.cpu().tolist())\n","                    preds.extend(torch.argmax(graphic_logits,dim=-1).cpu().tolist())\"\"\"\n","                cpu_acc=sum([int(i==j) for i,j in zip(cpu_preds, cpu_labels)])/len(cpu_preds)\n","                graphic_acc=sum([int(i==j) for i,j in zip(graphic_preds, graphic_labels)])/len(graphic_preds)\n","                hard_acc=sum([int(i==j) for i,j in zip(hard_preds, hard_labels)])/len(hard_preds)\n","                ram_acc=sum([int(i==j) for i,j in zip(ram_preds, ram_labels)])/len(ram_preds)\n","                scre_acc=sum([int(i==j) for i,j in zip(scre_preds, scre_labels)])/len(scre_preds)\n","\n","            save_path=\"/kaggle/working/mixprompt_all_epoch_\"+str(i+1)+\"_test_res.csv\"\n","            n=len(cpu_labels)\n","            record=[]\n","            for j in range(0,n):\n","                tmp={\"index\":j, \"cpu_label\":cpu_labels[j], \"cpu_prediction\":cpu_preds[j], \"cpu_all_pred\": cpu_all_pred[j],\\\n","                                \"graphic_label\":graphic_labels[j], \"graphic_prediction\":graphic_preds[j], \"graphic_all_pred\": graphic_all_pred[j],\\\n","                                \"hard_label\":hard_labels[j], \"hard_prediction\":hard_preds[j], \"hard_all_pred\": hard_all_pred[j],\\\n","                                \"ram_label\":ram_labels[j], \"ram_prediction\":ram_preds[j], \"ram_all_pred\": ram_all_pred[j],\\\n","                                \"screen_label\":scre_labels[j], \"screen_prediction\":scre_preds[j], \"screen_all_pred\": scre_all_pred[j]}\n","                record.append(tmp)\n","\n","            with open(save_path, 'w', newline='') as csvfile:\n","                fieldnames = ['index', 'cpu_label','cpu_prediction','cpu_all_pred',\\\n","                             'graphic_label','graphic_prediction','graphic_all_pred',\\\n","                             'hard_label','hard_prediction','hard_all_pred',\\\n","                             'ram_label','ram_prediction','ram_all_pred',\\\n","                             'screen_label','screen_prediction','screen_all_pred']\n","                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","\n","                writer.writeheader()\n","                writer.writerows(record)\n","            print(save_path)\n","            \"\"\"print(i,\" epoch test accuracy is : \",acc)\"\"\"\n","            print(i,\" epoch test cpu accuracy is : \",cpu_acc)\n","            print(i,\" epoch test graphic card accuracy is : \",graphic_acc)\n","            print(i,\" epoch test hard disk accuracy is : \",hard_acc)\n","            print(i,\" epoch test ram accuracy is : \",ram_acc)\n","            print(i,\" epoch test scre accuracy is : \",scre_acc)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
